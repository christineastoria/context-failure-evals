{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Distraction: How Long Context Histories Degrade Recall Accuracy\n",
    "\n",
    "Context distraction is a **real production problem** affecting agents that perform complex, multi-step tasks.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "As LLM agents perform research tasks with many operations, each tool call and result accumulates in the conversation context. With complex tasks requiring dozens of tool calls, the context becomes extremely long. The Berkeley Function-Calling Leaderboard and recent research show that **LLMs struggle to maintain recall accuracy over very long contexts**.\n",
    "\n",
    "**Context Distraction Definition:** When accumulated tool call results and intermediate outputs across many steps overwhelm the LLM, causing it to lose track of specific information from earlier steps, leading to degraded recall accuracy.\n",
    "\n",
    "The challenge: Agents need to complete complex tasks requiring many steps, but each step adds context that can bury important details.\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "In this notebook, we'll use a multi-domain investment research task to:\n",
    "1. **Identify** how context length degrades recall accuracy in a standard agent\n",
    "2. **Measure** the impact on recall accuracy using real research tasks\n",
    "3. **Compare** two approaches: standard agent vs. optimized graph agent with context isolation\n",
    "4. **Validate** improvements through comprehensive evaluations\n",
    "\n",
    "The goal: Build agents that maintain high recall accuracy even across very long, complex task sequences.\n",
    "\n",
    "## Two Agent Approaches We'll Compare\n",
    "\n",
    "**Standard Agent** - Simple ReAct loop\n",
    "- All tool call results accumulate in context\n",
    "- No separation between planning and execution\n",
    "- Context grows linearly with number of steps\n",
    "\n",
    "**Graph Agent** - Optimized with context isolation\n",
    "- Uses supervisor/researcher pattern to isolate context\n",
    "- Reflection tools maintain plans over long tasks\n",
    "- Critical information passed explicitly between nodes\n",
    "- Context managed strategically to preserve important details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Import our test infrastructure\n",
    "from context_distraction.resources.test_tasks import TEST_TASKS\n",
    "from context_distraction.tests.evaluators import (\n",
    "    recall_accuracy_evaluator,\n",
    "    tool_call_completeness_evaluator,\n",
    "    tool_call_efficiency_evaluator,\n",
    "    extract_answers_json_from_text,\n",
    ")\n",
    "from context_distraction.tests.setup_datasets import build_reference_outputs\n",
    "from context_distraction.resources.validation_utils import extract_tool_calls_from_message\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Research Task\n",
    "\n",
    "We'll evaluate agents on a complex investment analysis task covering 5 technology sectors:\n",
    "- Renewable energy\n",
    "- Artificial intelligence\n",
    "- Electric vehicles\n",
    "- Quantum computing\n",
    "- Biotechnology\n",
    "\n",
    "Each domain requires gathering statistics, expert opinions, case studies, and performing financial calculations including:\n",
    "- Compound growth projections (10-year)\n",
    "- Cost-benefit analysis with NPV calculations\n",
    "- Correlation analyses\n",
    "- Investment portfolio optimization\n",
    "\n",
    "The agent must then answer 9 specific questions requiring precise recall of facts from throughout the research process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show test case 1 structure\n",
    "task_1 = TEST_TASKS[0]\n",
    "\n",
    "print(f\"Task: {task_1['name']}\")\n",
    "print(f\"\\nDomains: {', '.join(task_1['topics'])}\")\n",
    "print(f\"\\nExpected operations per domain:\")\n",
    "print(f\"  - Statistics: {task_1['stats_count']}\")\n",
    "print(f\"  - Expert opinions: {task_1['expert_count']}\")\n",
    "print(f\"  - Case studies: {task_1['case_count']}\")\n",
    "print(f\"  - Historical years: {task_1['year_count']}\")\n",
    "print(f\"  - Domain comparisons: {task_1['compare_count']}\")\n",
    "print(f\"\\nTotal operations: ~40-50 tool calls\")\n",
    "print(f\"Recall questions: {len(task_1['recall_questions'])}\")\n",
    "print(f\"\\nSample questions:\")\n",
    "for i, q in enumerate(task_1['recall_questions'][:3], 1):\n",
    "    print(f\"  {i}. {q.split('(Expected')[0].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Runner Functions\n",
    "\n",
    "These functions run each agent type and extract structured outputs for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import agents\n",
    "from context_distraction.agent import agent as standard_agent\n",
    "from context_distraction.graph import graph as graph_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "async def run_standard_agent(query: str) -> dict:\n",
    "    \"\"\"Run standard agent and extract trajectory using streaming.\"\"\"\n",
    "    trajectory = []\n",
    "    final_response = \"\"\n",
    "    all_messages = []\n",
    "    \n",
    "    async for chunk in standard_agent.astream(\n",
    "        {\"messages\": [(\"user\", query)]},\n",
    "        stream_mode=\"updates\",\n",
    "    ):\n",
    "        if isinstance(chunk, tuple) and len(chunk) >= 2:\n",
    "            namespace, data = chunk\n",
    "        elif isinstance(chunk, dict):\n",
    "            data = chunk\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for key in ['tools', 'model']:\n",
    "                if key in data:\n",
    "                    msgs = data[key].get('messages', [])\n",
    "                    all_messages.extend(msgs)\n",
    "                    \n",
    "                    for msg in msgs:\n",
    "                        tool_calls = extract_tool_calls_from_message(msg)\n",
    "                        for tc in tool_calls:\n",
    "                            trajectory.append(tc)\n",
    "    \n",
    "    # Extract final response\n",
    "    for msg in reversed(all_messages):\n",
    "        if isinstance(msg, dict) and msg.get(\"content\"):\n",
    "            final_response = msg[\"content\"]\n",
    "            break\n",
    "        elif hasattr(msg, 'content') and msg.content:\n",
    "            final_response = msg.content\n",
    "            break\n",
    "    \n",
    "    return {\"final_response\": final_response, \"trajectory\": trajectory}\n",
    "\n",
    "\n",
    "async def run_graph_agent(query: str) -> dict:\n",
    "    \"\"\"Run graph agent and extract outputs using streaming.\"\"\"\n",
    "    trajectory = []\n",
    "    final_response = \"\"\n",
    "    all_messages = []\n",
    "    \n",
    "    config = {\"recursion_limit\": 200}\n",
    "    \n",
    "    async for chunk in graph_agent.astream(\n",
    "        {\"supervisor_messages\": [HumanMessage(content=query)]},\n",
    "        config=config,\n",
    "        subgraphs=True,\n",
    "        stream_mode=\"updates\",\n",
    "    ):\n",
    "        if isinstance(chunk, tuple) and len(chunk) >= 2:\n",
    "            namespace, data = chunk\n",
    "        elif isinstance(chunk, dict):\n",
    "            data = chunk\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for node_key, node_data in data.items():\n",
    "                if isinstance(node_data, dict):\n",
    "                    for msg_key in ['supervisor_messages', 'reseacher_messages', 'messages']:\n",
    "                        if msg_key in node_data and isinstance(node_data[msg_key], list):\n",
    "                            msgs = node_data[msg_key]\n",
    "                            all_messages.extend(msgs)\n",
    "                            \n",
    "                            for msg in msgs:\n",
    "                                tool_calls = extract_tool_calls_from_message(msg)\n",
    "                                for tc in tool_calls:\n",
    "                                    trajectory.append(tc)\n",
    "    \n",
    "    # Extract final response\n",
    "    for msg in reversed(all_messages):\n",
    "        if isinstance(msg, dict) and msg.get(\"content\"):\n",
    "            final_response = msg[\"content\"]\n",
    "            break\n",
    "        elif hasattr(msg, 'content') and msg.content:\n",
    "            final_response = msg.content\n",
    "            break\n",
    "    \n",
    "    return {\"final_response\": final_response, \"trajectory\": trajectory}\n",
    "\n",
    "print(\"‚úì Defined agent runners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Case 1 on Both Agents\n",
    "\n",
    "Let's run the first test case on both agents and evaluate their performance.\n",
    "\n",
    "**Note:** This will take several minutes as the agents conduct comprehensive research across 5 domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test case 1\n",
    "task = TEST_TASKS[0]\n",
    "reference_outputs = build_reference_outputs(task)\n",
    "inputs = {\"query\": task[\"query\"]}\n",
    "\n",
    "print(f\"Running Test Case 1: {task['name']}\")\n",
    "print(f\"This involves {len(task['topics'])} domains and ~40-50 tool calls\\n\")\n",
    "\n",
    "print(\"Running standard agent...\")\n",
    "standard_outputs = await run_standard_agent(task[\"query\"])\n",
    "print(f\"  ‚úì Completed {len(standard_outputs['trajectory'])} tool calls\\n\")\n",
    "\n",
    "print(\"Running graph agent...\")\n",
    "graph_outputs = await run_graph_agent(task[\"query\"])\n",
    "print(f\"  ‚úì Completed {len(graph_outputs['trajectory'])} tool calls\\n\")\n",
    "\n",
    "print(\"‚úì Both agents completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "\n",
    "Let's evaluate both agents on:\n",
    "1. **Recall Accuracy** - Can the agent recall specific facts from throughout the research?\n",
    "2. **Tool Call Completeness** - Did the agent make all necessary tool calls?\n",
    "3. **Tool Call Efficiency** - How many extra tool calls were made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate standard agent\n",
    "standard_recall = recall_accuracy_evaluator(inputs, standard_outputs, reference_outputs)\n",
    "standard_completeness = tool_call_completeness_evaluator(inputs, standard_outputs, reference_outputs)\n",
    "standard_efficiency = tool_call_efficiency_evaluator(inputs, standard_outputs, reference_outputs)\n",
    "\n",
    "print(\"Standard Agent Results:\")\n",
    "print(f\"  Recall Accuracy: {standard_recall['score']:.1%}\")\n",
    "print(f\"  Tool Call Completeness: {standard_completeness['score']:.1%}\")\n",
    "print(f\"  Tool Call Efficiency: {standard_efficiency['score']:.2f}\")\n",
    "print(f\"\\n{standard_recall['comment']}\\n\")\n",
    "\n",
    "# Evaluate graph agent\n",
    "graph_recall = recall_accuracy_evaluator(inputs, graph_outputs, reference_outputs)\n",
    "graph_completeness = tool_call_completeness_evaluator(inputs, graph_outputs, reference_outputs)\n",
    "graph_efficiency = tool_call_efficiency_evaluator(inputs, graph_outputs, reference_outputs)\n",
    "\n",
    "print(\"Graph Agent Results:\")\n",
    "print(f\"  Recall Accuracy: {graph_recall['score']:.1%}\")\n",
    "print(f\"  Tool Call Completeness: {graph_completeness['score']:.1%}\")\n",
    "print(f\"  Tool Call Efficiency: {graph_efficiency['score']:.2f}\")\n",
    "print(f\"\\n{graph_recall['comment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Comparison\n",
    "\n",
    "Let's create charts to visualize the performance difference between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison data\n",
    "agents = [\"Standard Agent\", \"Graph Agent\\n(Context Isolation)\"]\n",
    "recall_scores = [standard_recall['score'], graph_recall['score']]\n",
    "completeness_scores = [standard_completeness['score'], graph_completeness['score']]\n",
    "efficiency_scores = [standard_efficiency['score'], graph_efficiency['score']]\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Recall Accuracy',\n",
    "    x=agents,\n",
    "    y=recall_scores,\n",
    "    marker_color='#1f77b4',\n",
    "    text=[f\"{s:.1%}\" for s in recall_scores],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Tool Call Completeness',\n",
    "    x=agents,\n",
    "    y=completeness_scores,\n",
    "    marker_color='#2ca02c',\n",
    "    text=[f\"{s:.1%}\" for s in completeness_scores],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Tool Call Efficiency',\n",
    "    x=agents,\n",
    "    y=efficiency_scores,\n",
    "    marker_color='#ff7f0e',\n",
    "    text=[f\"{s:.2f}\" for s in efficiency_scores],\n",
    "    textposition='outside'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Standard Agent vs Graph Agent with Context Isolation\",\n",
    "    yaxis_title=\"Score\",\n",
    "    barmode='group',\n",
    "    height=500,\n",
    "    yaxis=dict(range=[0, 1.1]),\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create detailed comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Agent\": agents,\n",
    "    \"Recall Accuracy\": [f\"{s:.1%}\" for s in recall_scores],\n",
    "    \"Tool Call Completeness\": [f\"{s:.1%}\" for s in completeness_scores],\n",
    "    \"Tool Call Efficiency\": [f\"{s:.2f}\" for s in efficiency_scores],\n",
    "    \"Total Tool Calls\": [\n",
    "        len(standard_outputs['trajectory']),\n",
    "        len(graph_outputs['trajectory'])\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nüìä Key Findings:\")\n",
    "recall_improvement = (graph_recall['score'] - standard_recall['score']) * 100\n",
    "if recall_improvement > 0:\n",
    "    print(f\"   Graph agent improves recall accuracy by {recall_improvement:.1f} percentage points\")\n",
    "else:\n",
    "    print(f\"   Standard agent has higher recall by {-recall_improvement:.1f} percentage points\")\n",
    "\n",
    "completeness_improvement = (graph_completeness['score'] - standard_completeness['score']) * 100\n",
    "if completeness_improvement > 0:\n",
    "    print(f\"   Graph agent improves tool call completeness by {completeness_improvement:.1f} percentage points\")\n",
    "else:\n",
    "    print(f\"   Standard agent has higher completeness by {-completeness_improvement:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Recall Analysis\n",
    "\n",
    "Let's examine which specific questions each agent got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract answers from both agents\n",
    "standard_answers = extract_answers_json_from_text(standard_outputs['final_response'])\n",
    "graph_answers = extract_answers_json_from_text(graph_outputs['final_response'])\n",
    "expected_answers = reference_outputs['expected_answers']\n",
    "\n",
    "# Create question-by-question comparison\n",
    "from context_distraction.resources.validation_utils import compare_values\n",
    "\n",
    "questions_data = []\n",
    "for i in range(1, len(expected_answers) + 1):\n",
    "    expected = expected_answers.get(i) or expected_answers.get(str(i))\n",
    "    standard_answer = standard_answers.get(str(i)) or standard_answers.get(i)\n",
    "    graph_answer = graph_answers.get(str(i)) or graph_answers.get(i)\n",
    "    \n",
    "    standard_correct = compare_values(standard_answer, expected) if expected else False\n",
    "    graph_correct = compare_values(graph_answer, expected) if expected else False\n",
    "    \n",
    "    questions_data.append({\n",
    "        \"Question\": f\"Q{i}\",\n",
    "        \"Expected\": str(expected) if expected else \"N/A\",\n",
    "        \"Standard Agent\": str(standard_answer) if standard_answer else \"Missing\",\n",
    "        \"Standard ‚úì\": \"‚úì\" if standard_correct else \"‚úó\",\n",
    "        \"Graph Agent\": str(graph_answer) if graph_answer else \"Missing\",\n",
    "        \"Graph ‚úì\": \"‚úì\" if graph_correct else \"‚úó\"\n",
    "    })\n",
    "\n",
    "questions_df = pd.DataFrame(questions_data)\n",
    "display(questions_df)\n",
    "\n",
    "print(\"\\nüìù Question Analysis:\")\n",
    "standard_correct = sum(1 for q in questions_data if q[\"Standard ‚úì\"] == \"‚úì\")\n",
    "graph_correct = sum(1 for q in questions_data if q[\"Graph ‚úì\"] == \"‚úì\")\n",
    "print(f\"   Standard Agent: {standard_correct}/{len(expected_answers)} correct\")\n",
    "print(f\"   Graph Agent: {graph_correct}/{len(expected_answers)} correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the Graph Agent Works Better\n",
    "\n",
    "The graph agent addresses context distraction through two key innovations:\n",
    "\n",
    "### 1. Context Isolation\n",
    "\n",
    "The graph agent uses a **supervisor/researcher pattern** where:\n",
    "- **Supervisor node**: Maintains high-level plan and coordinates research\n",
    "- **Researcher nodes**: Execute specific research tasks in isolated contexts\n",
    "- **Explicit information passing**: Critical findings passed between nodes, not accumulated in one massive context\n",
    "\n",
    "This prevents the LLM from being overwhelmed by accumulated tool call results.\n",
    "\n",
    "### 2. Reflection Tools for Long Tasks\n",
    "\n",
    "The graph agent includes reflection mechanisms:\n",
    "- **Plan tracking**: Maintains structured task list of what needs to be done\n",
    "- **Progress checking**: Regularly reviews what's been completed\n",
    "- **Information synthesis**: Explicitly extracts and preserves key facts\n",
    "\n",
    "These tools help the agent maintain focus and recall accuracy even as the task becomes complex.\n",
    "\n",
    "### The Result\n",
    "\n",
    "By isolating context and using reflection tools, the graph agent can:\n",
    "- Complete more complex tasks without losing track of details\n",
    "- Maintain higher recall accuracy for facts from early research steps\n",
    "- Provide more complete and accurate final reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Context Distraction Is Real\n",
    "\n",
    "Our experiments demonstrate that:\n",
    "- **Standard agents struggle with long contexts**: As tasks become complex with many tool calls, recall accuracy degrades\n",
    "- **Simple facts get lost**: Even basic recall questions about early research steps are missed\n",
    "- **The problem compounds**: More steps = worse performance\n",
    "\n",
    "### Context Isolation Solves It\n",
    "\n",
    "The graph agent's approach delivers measurable improvements:\n",
    "- **Higher recall accuracy**: Better retention of facts from throughout the research process\n",
    "- **More complete execution**: Fewer missed research steps\n",
    "- **Maintained performance**: Scales better to complex, multi-step tasks\n",
    "\n",
    "### Professional Implementation Strategy\n",
    "\n",
    "When building agents for complex, multi-step tasks:\n",
    "\n",
    "1. **Recognize the threshold** - Simple tasks work fine with standard agents. Complex tasks (40+ tool calls) need context management.\n",
    "\n",
    "2. **Use graph patterns** - Supervisor/worker patterns isolate context and enable explicit information flow.\n",
    "\n",
    "3. **Add reflection tools** - Plan tracking, progress checking, and synthesis tools maintain accuracy over long sequences.\n",
    "\n",
    "4. **Measure with evaluations** - Use LangSmith to quantify recall accuracy, completeness, and efficiency.\n",
    "\n",
    "5. **Optimize for your domain** - Find the right balance of context isolation and information passing for your specific use case.\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "By addressing context distraction:\n",
    "- **Handle complex tasks** - Complete research/analysis tasks that standard agents can't\n",
    "- **Higher accuracy** - Maintain recall of specific facts and details\n",
    "- **Better UX** - Provide complete, accurate answers users can trust\n",
    "- **Production-ready** - Build agents that scale to real-world complexity\n",
    "\n",
    "---\n",
    "\n",
    "**Context distraction isn't solved by better prompts - it requires architectural patterns that manage context strategically. Use graph-based agents with context isolation for complex, multi-step tasks.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
