{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Distraction: When Long Context Histories Overwhelm LLM Agents\n",
    "\n",
    "Context distraction is a **real production problem** affecting agents with long conversation histories.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "As agents perform multi-step tasks, each step generates information that accumulates in the context. With many steps, the context becomes extremely long, and LLMs struggle to maintain accuracy when recalling information from early steps.\n",
    "\n",
    "**Context Distraction Definition:** When a simple tool-calling LLM agent accumulates so much context history across many steps that it overwhelms the LLM and causes it to lose track of information, leading to degraded recall accuracy and task completion.\n",
    "\n",
    "The challenge: Agents need to perform complex, multi-step tasks, but each step adds to context length. Eventually, the LLM can't effectively recall specific details from earlier steps.\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "In this notebook, we'll use a research assistant agent to:\n",
    "1. **Demonstrate** how context length grows with multi-step research tasks\n",
    "2. **Measure** recall accuracy degradation as context length increases\n",
    "3. **Identify** the point where context distraction significantly impacts performance\n",
    "4. **Evaluate** different strategies for managing long contexts\n",
    "\n",
    "The goal: Understand context distraction and develop strategies to maintain accuracy across long task sequences.\n",
    "\n",
    "## Research Tasks We'll Test\n",
    "\n",
    "We'll evaluate agents on research tasks of varying complexity:\n",
    "- **Small tasks**: 2-3 topics, ~8-12 steps\n",
    "- **Medium tasks**: 4-6 topics, ~16-24 steps  \n",
    "- **Large tasks**: 8-10 topics, ~32-40 steps\n",
    "- **Very large tasks**: 10+ topics with deep analysis, ~50+ steps\n",
    "\n",
    "Each step generates verbose, detailed information that must be recalled accurately later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EXAMPLE_RESEARCH_TASKS' from 'context_distraction.agent' (/Users/robertxu/Desktop/Projects/education/context-failure-evals/context_distraction/agent.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontext_distraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     all_research_tools,\n\u001b[32m     24\u001b[39m     core_research_tools,\n\u001b[32m     25\u001b[39m     analysis_tools\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontext_distraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minstructions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     RESEARCH_ASSISTANT_INSTRUCTIONS,\n\u001b[32m     29\u001b[39m     DETAILED_RESEARCH_INSTRUCTIONS\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontext_distraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     EXAMPLE_RESEARCH_TASKS,\n\u001b[32m     33\u001b[39m     RECALL_TEST_QUESTIONS,\n\u001b[32m     34\u001b[39m     create_focused_agent,\n\u001b[32m     35\u001b[39m     create_full_research_agent\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Initialize LangSmith\u001b[39;00m\n\u001b[32m     39\u001b[39m client = Client()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'EXAMPLE_RESEARCH_TASKS' from 'context_distraction.agent' (/Users/robertxu/Desktop/Projects/education/context-failure-evals/context_distraction/agent.py)"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from typing import List, Dict, Any, Literal\n",
    "from langchain.agents import create_agent\n",
    "from langsmith import Client, evaluate, traceable\n",
    "from langsmith.schemas import Run, Example\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Our research assistant agent tools\n",
    "from context_distraction.tools import (\n",
    "    all_research_tools,\n",
    "    core_research_tools,\n",
    "    analysis_tools\n",
    ")\n",
    "from context_distraction.instructions import (\n",
    "    RESEARCH_ASSISTANT_INSTRUCTIONS,\n",
    "    DETAILED_RESEARCH_INSTRUCTIONS\n",
    ")\n",
    "from context_distraction.agent import (\n",
    "    EXAMPLE_RESEARCH_TASKS,\n",
    "    RECALL_TEST_QUESTIONS,\n",
    "    create_focused_agent,\n",
    "    create_full_research_agent\n",
    ")\n",
    "\n",
    "# Initialize LangSmith\n",
    "client = Client()\n",
    "\n",
    "print(\"âœ“ Setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM - using OpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(f\"Using model: gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agents\n",
    "\n",
    "We'll compare two agents:\n",
    "1. **Standard Agent**: Uses `create_agent` - accumulates all tool results in context\n",
    "2. **Deep Agent**: Uses `create_deep_agent` - stores tool results in filesystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents\n",
    "standard_agent = create_standard_agent(llm)\n",
    "deep_agent = create_deep_agent_instance(llm)\n",
    "\n",
    "print(f\"âœ“ Created standard agent ({len(all_research_tools)} tools)\")\n",
    "print(f\"âœ“ Created deep agent ({len(all_research_tools)} tools)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Research Agent\n",
    "\n",
    "We'll create a research agent with full capabilities to conduct comprehensive multi-topic research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create research agent with all tools\n",
    "research_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=all_research_tools,\n",
    "    system_prompt=DETAILED_RESEARCH_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "print(f\"Research agent: {len(all_research_tools)} tools available\")\n",
    "print(f\"Core research tools: {len(core_research_tools)}\")\n",
    "print(f\"Analysis tools: {len(analysis_tools)}\")\n",
    "print(\"\\nThis agent can conduct comprehensive research but will accumulate\")\n",
    "print(\"extensive context across many research steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset: Research tasks of varying complexity\n",
    "test_tasks = [\n",
    "    {\n",
    "        \"name\": \"Small Task (2 topics)\",\n",
    "        \"query\": \"Research renewable energy and electric vehicles, then create a report comparing their growth and synergies.\",\n",
    "        \"topics\": [\"renewable_energy\", \"electric_vehicles\"],\n",
    "        \"expected_steps\": 8,\n",
    "        \"recall_questions\": [\n",
    "            \"What was the global installed capacity in gigawatts for renewable energy?\",\n",
    "            \"What was the battery cost per kWh for electric vehicles?\",\n",
    "            \"What is the name of an expert you consulted for renewable energy?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Medium Task (4 topics)\",\n",
    "        \"query\": \"Research renewable energy, artificial intelligence, climate change, and electric vehicles. Synthesize findings into a comprehensive report.\",\n",
    "        \"topics\": [\"renewable_energy\", \"artificial_intelligence\", \"climate_change\", \"electric_vehicles\"],\n",
    "        \"expected_steps\": 16,\n",
    "        \"recall_questions\": [\n",
    "            \"What was the global installed capacity in gigawatts for renewable energy?\",\n",
    "            \"What is the global AI market size in billions of USD?\",\n",
    "            \"What was the temperature increase in degrees Celsius since pre-industrial times?\",\n",
    "            \"What was the battery cost per kWh for electric vehicles?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Large Task (6 topics)\",\n",
    "        \"query\": \"Conduct comprehensive research on renewable energy, AI, climate change, quantum computing, biotechnology, and space exploration. Create a detailed report.\",\n",
    "        \"topics\": [\"renewable_energy\", \"artificial_intelligence\", \"climate_change\", \"quantum_computing\", \"biotechnology\", \"space_exploration\"],\n",
    "        \"expected_steps\": 24,\n",
    "        \"recall_questions\": [\n",
    "            \"What was the global installed capacity in gigawatts for renewable energy?\",\n",
    "            \"How many parameters does GPT-4 have according to your research?\",\n",
    "            \"What was the temperature increase in degrees Celsius?\",\n",
    "            \"How many qubits does IBM's Condor processor have?\",\n",
    "            \"What is the global biotechnology market size in billions of USD?\",\n",
    "            \"How many satellites were launched in 2023?\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Very Large Task (10 topics)\",\n",
    "        \"query\": \"Research all major technology domains: renewable energy, AI, climate change, quantum computing, biotechnology, space exploration, cybersecurity, electric vehicles, nanotechnology, and blockchain. Synthesize into a comprehensive report with specific statistics, expert opinions, and case studies.\",\n",
    "        \"topics\": [\n",
    "            \"renewable_energy\", \"artificial_intelligence\", \"climate_change\",\n",
    "            \"quantum_computing\", \"biotechnology\", \"space_exploration\",\n",
    "            \"cybersecurity\", \"electric_vehicles\", \"nanotechnology\", \"blockchain\"\n",
    "        ],\n",
    "        \"expected_steps\": 40,\n",
    "        \"recall_questions\": [\n",
    "            \"What was the global installed capacity in gigawatts for renewable energy?\",\n",
    "            \"What is the global AI market size in billions of USD?\",\n",
    "            \"What was the temperature increase in degrees Celsius?\",\n",
    "            \"How many qubits does IBM's Condor processor have?\",\n",
    "            \"What is the global biotechnology market size?\",\n",
    "            \"How many satellites were launched in 2023?\",\n",
    "            \"What is the global cybersecurity market size in billions of USD?\",\n",
    "            \"What was the battery cost per kWh for electric vehicles?\",\n",
    "            \"What is the global nanotechnology market size?\",\n",
    "            \"What is the cryptocurrency market cap in billions of USD?\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Test dataset: {len(test_tasks)} tasks\")\n",
    "for task in test_tasks:\n",
    "    print(f\"\\n{task['name']}: {len(task['topics'])} topics, ~{task['expected_steps']} steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset and Evaluators\n",
    "\n",
    "We'll create a LangSmith dataset with our research tasks and define evaluators to measure:\n",
    "- **Recall accuracy** - Can the agent recall specific details from early research steps?\n",
    "- **Context length** - How much context accumulates across steps\n",
    "- **Task completion** - Does the agent complete all required research steps?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall accuracy evaluator\n",
    "def recall_accuracy_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    final_response = run.outputs.get(\"final_response\", \"\")\n",
    "    recall_questions = example.outputs.get(\"recall_questions\", [])\n",
    "    \n",
    "    if not recall_questions or not final_response:\n",
    "        return {\"key\": \"recall_accuracy\", \"score\": 0.0, \"comment\": \"Missing data\"}\n",
    "    \n",
    "    correct_count = 0\n",
    "    response_lower = final_response.lower()\n",
    "    \n",
    "    # Check for key facts\n",
    "    checks = [\n",
    "        (\"renewable energy\" in q.lower() and \"capacity\" in q.lower(), \"3372\" in final_response or \"3,372\" in final_response),\n",
    "        (\"battery cost\" in q.lower() or \"kwh\" in q.lower(), \"139\" in final_response),\n",
    "        (\"ai market\" in q.lower(), \"196.6\" in final_response or \"196\" in final_response),\n",
    "        (\"temperature increase\" in q.lower(), \"1.1\" in final_response),\n",
    "        (\"qubits\" in q.lower() or \"condor\" in q.lower(), \"1121\" in final_response or \"1,121\" in final_response),\n",
    "        (\"biotechnology\" in q.lower() and \"market\" in q.lower(), \"1023\" in final_response or \"1,023\" in final_response),\n",
    "        (\"satellites\" in q.lower() and \"2023\" in q.lower(), \"2877\" in final_response or \"2,877\" in final_response),\n",
    "        (\"cybersecurity\" in q.lower() and \"market\" in q.lower(), \"202\" in final_response),\n",
    "        (\"nanotechnology\" in q.lower() and \"market\" in q.lower(), \"75\" in final_response),\n",
    "        (\"cryptocurrency\" in q.lower() or \"crypto market\" in q.lower(), \"1200\" in final_response or \"1,200\" in final_response),\n",
    "    ]\n",
    "    \n",
    "    for question in recall_questions:\n",
    "        q_lower = question.lower()\n",
    "        found = False\n",
    "        for pattern, condition in checks:\n",
    "            if pattern and condition:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / len(recall_questions) if recall_questions else 0.0\n",
    "    return {\"key\": \"recall_accuracy\", \"score\": accuracy, \"comment\": f\"Recalled {correct_count}/{len(recall_questions)} facts\"}\n",
    "\n",
    "# Context length evaluator\n",
    "def context_length_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    trajectory = run.outputs.get(\"trajectory\", [])\n",
    "    final_response = run.outputs.get(\"final_response\", \"\")\n",
    "    tool_calls_count = len(trajectory)\n",
    "    estimated_context_chars = tool_calls_count * 10000 + len(final_response)\n",
    "    context_k_chars = estimated_context_chars / 1000\n",
    "    return {\"key\": \"context_length\", \"score\": context_k_chars, \"comment\": f\"{context_k_chars:.0f}K chars ({tool_calls_count} tool calls)\"}\n",
    "\n",
    "# Task completion evaluator\n",
    "def task_completion_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    final_response = run.outputs.get(\"final_response\", \"\")\n",
    "    topics = example.outputs.get(\"topics\", [])\n",
    "    trajectory = run.outputs.get(\"trajectory\", [])\n",
    "    \n",
    "    if not topics:\n",
    "        return {\"key\": \"task_completion\", \"score\": 0.0, \"comment\": \"No topics\"}\n",
    "    \n",
    "    researched_topics = set()\n",
    "    response_lower = final_response.lower()\n",
    "    \n",
    "    for topic in topics:\n",
    "        topic_key = topic.replace(\"_\", \" \")\n",
    "        if topic_key in response_lower or topic in response_lower:\n",
    "            researched_topics.add(topic)\n",
    "    \n",
    "    for tool_call in trajectory:\n",
    "        if tool_call.get(\"name\") == \"research_topic\":\n",
    "            topic_arg = tool_call.get(\"args\", {}).get(\"topic\", \"\")\n",
    "            if topic_arg:\n",
    "                researched_topics.add(topic_arg.replace(\" \", \"_\"))\n",
    "    \n",
    "    completion_rate = len(researched_topics) / len(topics) if topics else 0.0\n",
    "    return {\"key\": \"task_completion\", \"score\": completion_rate, \"comment\": f\"Researched {len(researched_topics)}/{len(topics)} topics\"}\n",
    "\n",
    "ALL_EVALUATORS = [recall_accuracy_evaluator, context_length_evaluator, task_completion_evaluator]\n",
    "print(f\"âœ“ Defined {len(ALL_EVALUATORS)} evaluators\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_agent_with_trajectory(agent, query: str) -> dict:\n",
    "    \"\"\"Run agent and return structured output.\"\"\"\n",
    "    result = agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "    \n",
    "    final_response = \"\"\n",
    "    trajectory = []\n",
    "    messages = result.get(\"messages\", [])\n",
    "    \n",
    "    for msg in messages:\n",
    "        if isinstance(msg, dict) and msg.get(\"type\") == \"ai\":\n",
    "            if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n",
    "                for tc in msg[\"tool_calls\"]:\n",
    "                    trajectory.append({\"name\": tc.get(\"name\", \"\"), \"args\": tc.get(\"args\", {})})\n",
    "            if msg.get(\"content\"):\n",
    "                final_response = msg[\"content\"]\n",
    "        elif hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                trajectory.append({\n",
    "                    \"name\": tc.name if hasattr(tc, 'name') else tc.get(\"name\", \"\"),\n",
    "                    \"args\": tc.args if hasattr(tc, 'args') else tc.get(\"args\", {})\n",
    "                })\n",
    "        if hasattr(msg, 'content') and msg.content:\n",
    "            final_response = msg.content\n",
    "    \n",
    "    return {\"final_response\": final_response, \"trajectory\": trajectory}\n",
    "\n",
    "print(\"âœ“ Created agent wrapper\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations\n",
    "print(\"Running evaluations...\\n\")\n",
    "\n",
    "standard_experiment = evaluate(\n",
    "    lambda inputs: run_research_agent_with_trajectory(standard_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"context-distraction-standard\",\n",
    "    metadata={\"agent_type\": \"standard\"},\n",
    ")\n",
    "\n",
    "deep_experiment = evaluate(\n",
    "    lambda inputs: run_research_agent_with_trajectory(deep_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"context-distraction-deep\",\n",
    "    metadata={\"agent_type\": \"deep\"},\n",
    ")\n",
    "\n",
    "print(\"âœ“ Evaluations complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "Now let's run evaluations on tasks of increasing complexity to measure context distraction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on research agent\n",
    "print(\"Running evaluation on research tasks...\")\n",
    "print(\"This may take several minutes as the agent conducts comprehensive research.\\n\")\n",
    "\n",
    "research_experiment = evaluate(\n",
    "    lambda inputs: run_research_agent_with_trajectory(research_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"context-distraction-research\",\n",
    "    metadata={\"agent_type\": \"full_research\", \"tools\": len(all_research_tools)},\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation complete!\")\n",
    "print(f\"   View results: https://smith.langchain.com/o/{client._get_tenant_id()}/datasets/{dataset.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    standard_metrics = get_metrics_from_experiment(standard_experiment)\n",
    "    deep_metrics = get_metrics_from_experiment(deep_experiment)\n",
    "    \n",
    "    complexities = list(standard_metrics.keys())\n",
    "    standard_recall = [standard_metrics[c][\"recall_accuracy\"] for c in complexities]\n",
    "    deep_recall = [deep_metrics[c][\"recall_accuracy\"] for c in complexities]\n",
    "    standard_context = [standard_metrics[c][\"context_length\"] for c in complexities]\n",
    "    deep_context = [deep_metrics[c][\"context_length\"] for c in complexities]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Recall Accuracy\", \"Context Length\"),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=complexities, y=standard_recall, name=\"Standard\", marker_color='#d62728'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(x=complexities, y=deep_recall, name=\"Deep\", marker_color='#2ca02c'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(x=complexities, y=standard_context, name=\"Standard\", marker_color='#d62728', showlegend=False), row=1, col=2)\n",
    "    fig.add_trace(go.Bar(x=complexities, y=deep_context, name=\"Deep\", marker_color='#2ca02c', showlegend=False), row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Task Complexity\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Task Complexity\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Recall Accuracy\", row=1, col=1, range=[0, 1])\n",
    "    fig.update_yaxes(title_text=\"Context Length (K chars)\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(title_text=\"Standard Agent vs Deep Agent Comparison\", height=400, barmode='group')\n",
    "    fig.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Standard Agent**: Accumulates all tool results in context â†’ context distraction â†’ degraded recall\n",
    "\n",
    "**Deep Agent**: Stores tool results in filesystem â†’ reduced context â†’ maintained recall accuracy\n",
    "\n",
    "**Solution**: Dropping tool call results from context (via filesystem storage) mitigates context distraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of context distraction\n",
    "try:\n",
    "    metrics = get_metrics_from_experiment(research_experiment)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    complexities = list(metrics.keys())\n",
    "    recall_scores = [metrics[c][\"recall_accuracy\"] for c in complexities]\n",
    "    context_lengths = [metrics[c][\"context_length\"] for c in complexities]\n",
    "    completion_scores = [metrics[c][\"task_completion\"] for c in complexities]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Recall Accuracy vs Task Complexity\",\n",
    "            \"Context Length Growth\",\n",
    "            \"Task Completion Rate\",\n",
    "            \"Recall Accuracy vs Context Length\"\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Plot 1: Recall accuracy by complexity\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=complexities,\n",
    "            y=recall_scores,\n",
    "            name=\"Recall Accuracy\",\n",
    "            marker_color='#1f77b4',\n",
    "            text=[f\"{s:.1%}\" for s in recall_scores],\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 2: Context length growth\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=complexities,\n",
    "            y=context_lengths,\n",
    "            name=\"Context Length (K chars)\",\n",
    "            marker_color='#ff7f0e',\n",
    "            text=[f\"{c:.0f}K\" for c in context_lengths],\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Plot 3: Task completion\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=complexities,\n",
    "            y=completion_scores,\n",
    "            name=\"Task Completion\",\n",
    "            marker_color='#2ca02c',\n",
    "            text=[f\"{s:.1%}\" for s in completion_scores],\n",
    "            textposition='outside'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Plot 4: Recall vs Context Length (scatter)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=context_lengths,\n",
    "            y=recall_scores,\n",
    "            mode='lines+markers',\n",
    "            name=\"Recall Accuracy\",\n",
    "            marker=dict(size=12, color='#d62728'),\n",
    "            line=dict(width=3, color='#d62728')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text=\"Task Complexity\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Task Complexity\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Task Complexity\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Context Length (K chars)\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Recall Accuracy\", row=1, col=1, range=[0, 1])\n",
    "    fig.update_yaxes(title_text=\"Context Length (K chars)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Completion Rate\", row=2, col=1, range=[0, 1])\n",
    "    fig.update_yaxes(title_text=\"Recall Accuracy\", row=2, col=2, range=[0, 1])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Context Distraction: Performance Degradation Analysis\",\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Chart Analysis:\")\n",
    "    print(\"   - Recall Accuracy vs Complexity: Shows degradation as tasks get larger\")\n",
    "    print(\"   - Context Length Growth: Demonstrates linear/exponential growth\")\n",
    "    print(\"   - Task Completion: Indicates if agent completes all research steps\")\n",
    "    print(\"   - Recall vs Context Length: Reveals the distraction threshold\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating visualization: {e}\")\n",
    "    print(\"Run the evaluation above first to see charts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Context Distraction Findings\n",
    "\n",
    "Based on the evaluation results, we can observe:\n",
    "\n",
    "1. **Recall Accuracy Degrades**: As context length increases, the agent's ability to recall specific facts from early research steps decreases significantly.\n",
    "\n",
    "2. **Context Length Grows**: Each research step adds substantial context (~10K-20K chars per tool call), leading to very long contexts for large tasks.\n",
    "\n",
    "3. **Task Completion Suffers**: For very large tasks, the agent may miss some research steps or provide incomplete synthesis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
