{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Confusion: Finding the Right Balance for Production Agents\n",
    "\n",
    "Context confusion is a **real production problem** affecting agents today.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "With MCP making it easy to connect dozens of services, agents can access more tools than ever. But the Berkeley Function-Calling Leaderboard shows a concerning pattern: **every model performs worse when provided with more tools**.\n",
    "\n",
    "**Context Confusion Definition:** Superfluous content in the context from excessive tools, verbose instructions, or irrelevant information leads to low-quality responses.\n",
    "\n",
    "The goal is to strike the right balance between capability and clarity. Too few tools limits what your agent can do. Too many tools, or poorly organized context, degrades performance.\n",
    "\n",
    "## What We'll Explore\n",
    "\n",
    "In this notebook, we'll use a shipping support agent to:\n",
    "1. **Identify** how tool bloat, instruction overload, and irrelevant context degrade performance\n",
    "2. **Measure** the impact using LangSmith evaluations at each problem step\n",
    "3. **Solve** with three strategies: consolidation, smart routing, and pruning\n",
    "4. **Validate** improvements LangSmith experiments\n",
    "\n",
    "The goal: Build agents that are both capable and effective by optimizing what goes into the context.\n",
    "\n",
    "## Three Problems We'll Solve\n",
    "\n",
    "Using LangSmith evaluations, we'll diagnose and fix:\n",
    "\n",
    "**Problem 1: Overloaded tools** - Too many tools degrades quality and model decision making \n",
    "\n",
    "**Problem 2: Irrelevant Noise** - Unrelated tools distract the model, even with moderate tool counts\n",
    "\n",
    "**Problem 3: Instruction Bloat** - Verbose, multi-domain instructions reduce focus and accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from typing import List, Dict, Any, Literal\n",
    "from langchain.agents import create_agent\n",
    "from langsmith import Client, evaluate, traceable\n",
    "from langsmith.schemas import Run, Example\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Our shipping support agent tools\n",
    "from context_confusion.tools import (\n",
    "    shipping_core_tools,\n",
    "    carrier_tools, \n",
    "    returns_tools,\n",
    "    warehouse_tools,\n",
    "    order_modification_tools,\n",
    "    customer_service_tools,\n",
    "    billing_tools,\n",
    "    fraud_tools,\n",
    "    analytics_tools,\n",
    "    marketing_tools,\n",
    "    vendor_tools,\n",
    "    employee_tools,\n",
    "    quality_tools,\n",
    "    all_tools,\n",
    ")\n",
    "from context_confusion.instructions import (\n",
    "    SHIPPING_SUPPORT_INSTRUCTIONS\n",
    ")\n",
    "from context_confusion.additional_context import IRRELEVANT_INSTRUCTIONS\n",
    "\n",
    "# Initialize LangSmith\n",
    "client = Client()\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: claude-haiku-4-5-20251001\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM - using 2025 Haiku for context confusion demo\n",
    "llm = ChatAnthropic(model=\"claude-haiku-4-5-20251001\", temperature=0)\n",
    "\n",
    "print(f\"Using model: {llm.model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Production Agent: Capable but Struggling\n",
    "\n",
    "Meet our shipping support agent. Over time, we've added tools for every domain:\n",
    "- Core shipping operations (orders, tracking, shipments)\n",
    "- Returns and customer service\n",
    "- Warehouse and inventory management\n",
    "- Billing and fraud detection\n",
    "- Analytics and reporting\n",
    "- Marketing, vendor management, employee ops...\n",
    "\n",
    "**Total: ~75 tools across 18 domains**\n",
    "\n",
    "The agent can theoretically handle any shipping-related query. But something's wrong with production performance - context confusion from too many overlapping tool choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system prompt:  You are a helpful shipping support agent. \n",
      "Use the available tools to answer customer questions about orders, tracking, returns, and shipping.\n",
      "\n",
      "Key guidelines:\n",
      "- Always verify customer identity before sharing order details\n",
      "- Check for carrier incidents if there are delivery delays\n",
      "- Provide clear, accurate information based on the tools available\n",
      "\n",
      "Current date: December 20, 2025\n",
      "Production agent: 75 tools available\n",
      "This represents a realistic 'kitchen sink' approach where all capabilities are exposed.\n",
      "Includes confusing near-duplicates that cause tool selection ambiguity.\n"
     ]
    }
   ],
   "source": [
    "# Create production agent with all ~75 tools\n",
    "print(\"system prompt: \", SHIPPING_SUPPORT_INSTRUCTIONS)\n",
    "\n",
    "production_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=all_tools,\n",
    "    system_prompt=SHIPPING_SUPPORT_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Production agent: {len(all_tools)} tools available\")\n",
    "print(\"This represents a realistic 'kitchen sink' approach where all capabilities are exposed.\")\n",
    "print(\"Includes confusing near-duplicates that cause tool selection ambiguity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset\n",
    "\n",
    "10 realistic queries covering common shipping support scenarios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 10 test cases exposing context confusion patterns\n",
      "   - 4 duplicate tool confusion cases\n",
      "   - 3 multi-domain query cases\n",
      "   - 3 missing capability cases\n"
     ]
    }
   ],
   "source": [
    "# Test dataset - Designed to expose context confusion patterns\n",
    "test_cases = [\n",
    "    # ===== TYPE A: Duplicate Tool Confusion (4 cases) =====\n",
    "    # These queries trigger confusing duplicate tools in the 75-tool setup\n",
    "    \n",
    "    {\n",
    "        \"query\": \"What's the status of order #84721?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# order_status_provided\n",
    "The response should mention the status of order #84721\n",
    "\n",
    "# status_detail\n",
    "The response should indicate whether the order is in transit, delivered, or processing\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"84721\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"duplicate_tools\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Track order #99002 and tell me where it is\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# tracking_info_provided\n",
    "The response should provide location or tracking status for order #99002\n",
    "\n",
    "# current_location\n",
    "The response should mention where the package currently is or its delivery status\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"99002\"}},\n",
    "            {\"name\": \"get_tracking_details\", \"args\": {\"tracking_number\": \"TRK99002ABC\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"duplicate_tools\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Check order #98765 for me\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# order_info_provided\n",
    "The response should provide information about order #98765\n",
    "\n",
    "# essential_details\n",
    "The response should include at least one of: status, tracking, or delivery estimate\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"98765\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"duplicate_tools\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Where is order #12345 right now?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# current_location_provided\n",
    "The response should indicate the current location of order #12345\n",
    "\n",
    "# tracking_detail\n",
    "The response should mention a specific location or scanning facility\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"12345\"}},\n",
    "            {\"name\": \"get_tracking_details\", \"args\": {\"tracking_number\": \"TRK12345ABC\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"duplicate_tools\"\n",
    "    },\n",
    "    \n",
    "    # ===== TYPE B: Multi-Domain Queries (3 cases) =====\n",
    "    # Queries requiring information from multiple domains\n",
    "    \n",
    "    {\n",
    "        \"query\": \"I'm user@example.com, why is order #23456 delayed and when will it arrive?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# delay_reason_explained\n",
    "The response should explain why order #23456 is delayed\n",
    "\n",
    "# arrival_estimate_provided\n",
    "The response should provide or acknowledge the estimated delivery date\n",
    "\n",
    "# customer_identified\n",
    "The response should acknowledge the customer email if verification is needed\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_customer_by_email\", \"args\": {\"email\": \"user@example.com\"}},\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"23456\"}},\n",
    "            {\"name\": \"get_shipment\", \"args\": {\"order_id\": \"23456\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"multi_domain\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What carrier is handling order #45678 and are there any service disruptions?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# carrier_identified\n",
    "The response should mention which carrier is handling the order\n",
    "\n",
    "# service_status_checked\n",
    "The response should indicate whether there are carrier incidents or confirm no disruptions\n",
    "\n",
    "# order_verified\n",
    "The response should confirm the order exists and is being tracked\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"45678\"}},\n",
    "            {\"name\": \"get_shipment\", \"args\": {\"order_id\": \"45678\"}},\n",
    "            {\"name\": \"get_carrier_incidents\", \"args\": {\"date\": \"2024-03-15\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"multi_domain\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm ops@widget.io and I need to know the status and delivery preferences for order #10015\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# order_status_provided\n",
    "The response should include the status of order #10015\n",
    "\n",
    "# customer_verified\n",
    "The response should acknowledge customer identification\n",
    "\n",
    "# preferences_mentioned\n",
    "The response should mention or acknowledge delivery preferences if available\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_customer_by_email\", \"args\": {\"email\": \"ops@widget.io\"}},\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"10015\"}},\n",
    "            {\"name\": \"get_customer_preferences\", \"args\": {\"customer_id\": \"cust_widget\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"multi_domain\"\n",
    "    },\n",
    "    \n",
    "    # ===== TYPE C: Missing Capability (3 cases) =====\n",
    "    # Queries that 6-tool minimal agent cannot handle\n",
    "    \n",
    "    {\n",
    "        \"query\": \"I want to return order #11111 - what's the process?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# return_info_provided\n",
    "The response should provide information about returning order #11111\n",
    "\n",
    "# return_status_or_process\n",
    "The response should mention return status, process, or how to initiate a return\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_order\", \"args\": {\"order_id\": \"11111\"}},\n",
    "            {\"name\": \"get_return_request\", \"args\": {\"order_id\": \"11111\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"missing_capability\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are there any carrier service issues affecting my shipment today?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# carrier_incidents_checked\n",
    "The response should indicate whether there are carrier service incidents\n",
    "\n",
    "# current_date_considered\n",
    "The response should check incidents for today or recent date\n",
    "\n",
    "# relevant_info_provided\n",
    "The response should provide useful information about potential delays\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_carrier_incidents\", \"args\": {\"date\": \"2024-12-20\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"missing_capability\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm manager@euro-corp.eu, what are my delivery preferences?\",\n",
    "        \"success_criteria\": \"\"\"\n",
    "# customer_identified\n",
    "The response should verify the customer by email\n",
    "\n",
    "# preferences_retrieved\n",
    "The response should provide delivery preferences for the customer account\n",
    "\n",
    "# privacy_acknowledged\n",
    "The response may acknowledge customer privacy or account details\n",
    "\"\"\",\n",
    "        \"trajectory\": [\n",
    "            {\"name\": \"get_customer_by_email\", \"args\": {\"email\": \"manager@euro-corp.eu\"}},\n",
    "            {\"name\": \"get_customer_preferences\", \"args\": {\"customer_id\": \"cust_euro\"}}\n",
    "        ],\n",
    "        \"trajectory_comparison_mode\": \"subset\",\n",
    "        \"context_confusion_type\": \"missing_capability\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_cases)} test cases exposing context confusion patterns\")\n",
    "print(\"   - 4 duplicate tool confusion cases\")\n",
    "print(\"   - 3 multi-domain query cases\")\n",
    "print(\"   - 3 missing capability cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created dataset with 10 examples\n",
      "   View dataset: https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/46929a17-406b-4431-9eae-cd895dd2265c\n"
     ]
    }
   ],
   "source": [
    "# Create dataset in LangSmith with new structure\n",
    "dataset_name = \"christine-shipping-support-v7\"\n",
    "\n",
    "# Force recreate to ensure correct structure\n",
    "try:\n",
    "    existing = client.read_dataset(dataset_name=dataset_name)\n",
    "    client.delete_dataset(dataset_id=existing.id)\n",
    "    print(f\"âœ“ Deleted old dataset to create fresh version\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Test queries exposing context confusion patterns with success criteria\"\n",
    ")\n",
    "\n",
    "for case in test_cases:\n",
    "    client.create_example(\n",
    "        inputs={\"query\": case[\"query\"]},\n",
    "        outputs={\n",
    "            \"success_criteria\": case[\"success_criteria\"],\n",
    "            \"trajectory\": case[\"trajectory\"],\n",
    "            \"trajectory_comparison_mode\": case[\"trajectory_comparison_mode\"],\n",
    "            \"context_confusion_type\": case[\"context_confusion_type\"]\n",
    "        },\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ Created dataset with {len(test_cases)} examples\")\n",
    "print(f\"   View dataset: https://smith.langchain.com/o/{client._get_tenant_id()}/datasets/{dataset.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics\n",
    "\n",
    "Before we evaluate, let's define what we'll measure. These evaluators help us quantify context confusion:\n",
    "\n",
    "**Key Innovation**: Flexible trajectory comparison recognizes that consolidated tools can replace multiple specific tools while still accomplishing the same goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined compare_trajectory with flexible comparison modes\n"
     ]
    }
   ],
   "source": [
    "# Flexible trajectory comparison utility\n",
    "def compare_trajectory(tool_calls, expected_tool_calls, mode=\"strict\"):\n",
    "    \"\"\"\n",
    "    Compare tool call trajectories with multiple comparison modes.\n",
    "    Returns a score between 0.0 and 1.0 for partial credit.\n",
    "    \n",
    "    Modes:\n",
    "        strict: Exact match - same tools, same order\n",
    "        unordered: Same tools, any order\n",
    "        superset: Actual contains all expected (allows extras)\n",
    "        subset: Actual contains only expected tools (penalizes missing)\n",
    "    \n",
    "    This flexible comparison is critical for evaluating agents with different tool designs.\n",
    "    \"\"\"\n",
    "    def make_hashable(obj):\n",
    "        \"\"\"Recursively convert unhashable types to hashable ones.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))\n",
    "        elif isinstance(obj, list):\n",
    "            return tuple(make_hashable(item) for item in obj)\n",
    "        elif isinstance(obj, set):\n",
    "            return tuple(sorted(make_hashable(item) for item in obj))\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def normalize_tool_call(tc):\n",
    "        \"\"\"\n",
    "        Normalize a tool call for comparison.\n",
    "        Converts to a hashable tuple of (name, sorted_args).\n",
    "        \"\"\"\n",
    "        name = tc.get(\"name\", \"\")\n",
    "        args = tc.get(\"args\", {})\n",
    "        # Convert args to completely hashable structure (handles nested lists/dicts)\n",
    "        args_hashable = make_hashable(args)\n",
    "        return (name, args_hashable)\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if len(tool_calls) == 0 and len(expected_tool_calls) == 0:\n",
    "        return 1.0\n",
    "    if len(expected_tool_calls) == 0:\n",
    "        return 0.0 if len(tool_calls) > 0 else 1.0\n",
    "    if len(tool_calls) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize both trajectories\n",
    "    actual_normalized = [normalize_tool_call(tc) for tc in tool_calls]\n",
    "    expected_normalized = [normalize_tool_call(tc) for tc in expected_tool_calls]\n",
    "    \n",
    "    if mode == \"strict\":\n",
    "        # Exact match - same tools, same order\n",
    "        # Score = ratio of correct positions\n",
    "        matches = sum(1 for a, e in zip(actual_normalized, expected_normalized) if a == e)\n",
    "        max_len = max(len(actual_normalized), len(expected_normalized))\n",
    "        return matches / max_len if max_len > 0 else 0.0\n",
    "        \n",
    "    elif mode == \"unordered\":\n",
    "        # Same tools, any order - compare as multisets\n",
    "        from collections import Counter\n",
    "        actual_counter = Counter(actual_normalized)\n",
    "        expected_counter = Counter(expected_normalized)\n",
    "        \n",
    "        # Count matches (intersection)\n",
    "        matches = sum((actual_counter & expected_counter).values())\n",
    "        total_expected = len(expected_normalized)\n",
    "        return matches / total_expected if total_expected > 0 else 0.0\n",
    "        \n",
    "    elif mode == \"superset\":\n",
    "        # Actual must contain all expected (extras allowed)\n",
    "        # Score = ratio of expected tools found, penalized by extras\n",
    "        expected_set = set(expected_normalized)\n",
    "        actual_set = set(actual_normalized)\n",
    "        \n",
    "        # How many expected tools were called?\n",
    "        found = len(expected_set.intersection(actual_set))\n",
    "        expected_count = len(expected_set)\n",
    "        \n",
    "        if expected_count == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        # Base score: ratio of expected tools found\n",
    "        base_score = found / expected_count\n",
    "        \n",
    "        # Penalty for extra tools (noise)\n",
    "        extra = len(actual_set - expected_set)\n",
    "        noise_penalty = extra / (expected_count + extra) if (expected_count + extra) > 0 else 0\n",
    "        \n",
    "        return max(0.0, base_score - noise_penalty)\n",
    "        \n",
    "    elif mode == \"subset\":\n",
    "        # All actual tools must be in expected (no extras)\n",
    "        # Score = ratio of actual tools that are valid (in expected)\n",
    "        expected_set = set(expected_normalized)\n",
    "        actual_set = set(actual_normalized)\n",
    "        \n",
    "        # How many actual tools are valid (in expected)?\n",
    "        valid = len(actual_set.intersection(expected_set))\n",
    "        actual_count = len(actual_set)\n",
    "        \n",
    "        if actual_count == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Base score: ratio of valid actual tools\n",
    "        base_score = valid / actual_count\n",
    "        \n",
    "        # Additional check: did we call all expected tools?\n",
    "        expected_count = len(expected_set)\n",
    "        coverage = len(expected_set.intersection(actual_set)) / expected_count if expected_count > 0 else 1.0\n",
    "        \n",
    "        # Final score is average of validity and coverage\n",
    "        return (base_score + coverage) / 2.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "print(\"âœ“ Defined compare_trajectory with flexible comparison modes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined trajectory_match_evaluator with flexible comparison\n"
     ]
    }
   ],
   "source": [
    "# Flexible trajectory match evaluator\n",
    "def trajectory_match_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Flexible trajectory matching using comparison modes.\n",
    "    Returns scores from 0.0 to 1.0 for partial credit.\n",
    "    \n",
    "    Uses the mode specified in the example to handle different agent designs.\n",
    "    NO FALLBACKS - fails loudly if required data is missing.\n",
    "    \"\"\"\n",
    "    # NO .get() - fail loudly if missing\n",
    "    actual_trajectory = run.outputs[\"trajectory\"]\n",
    "    expected_trajectory = example.outputs[\"trajectory\"]\n",
    "    mode = example.outputs[\"trajectory_comparison_mode\"]\n",
    "    \n",
    "    # Use flexible comparison function\n",
    "    score = compare_trajectory(actual_trajectory, expected_trajectory, mode=mode)\n",
    "    \n",
    "    # Count noise tools (tools called that weren't expected)\n",
    "    actual_tools = [t[\"name\"] for t in actual_trajectory]\n",
    "    expected_tools = [t[\"name\"] for t in expected_trajectory]\n",
    "    noise_count = len([t for t in actual_tools if t not in expected_tools])\n",
    "    missing_count = len([t for t in expected_tools if t not in actual_tools])\n",
    "    \n",
    "    # Generate informative comment\n",
    "    if score == 1.0:\n",
    "        comment = f\"Perfect match! Called {len(actual_tools)} tools as expected.\"\n",
    "    elif score == 0.0:\n",
    "        comment = f\"No match. Called {len(actual_tools)} tools, expected {len(expected_tools)}. Missing: {missing_count}, Extra: {noise_count}\"\n",
    "    else:\n",
    "        comment = f\"Partial match ({score:.1%}). Called {len(actual_tools)} tools, expected {len(expected_tools)}. Missing: {missing_count}, Extra: {noise_count}\"\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"trajectory_match\",\n",
    "        \"score\": score,\n",
    "        \"comment\": comment\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Defined trajectory_match_evaluator with flexible comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined llm_trajectory_evaluator with improved prompting\n"
     ]
    }
   ],
   "source": [
    "# LLM judge for trajectory assessment with arguments \n",
    "class TrajectoryAssessment(TypedDict):\n",
    "    \"\"\"Evaluate tool call trajectory quality.\"\"\"\n",
    "    reasoning: Annotated[str, ..., \"Explain your assessment of the tool calls.\"]\n",
    "    is_appropriate: Annotated[bool, ..., \"True if tool calls and arguments are reasonable for the goal.\"]\n",
    "\n",
    "trajectory_judge = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "trajectory_judge_llm = trajectory_judge.with_structured_output(TrajectoryAssessment, method=\"function_calling\")\n",
    "\n",
    "def llm_trajectory_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM judge: Evaluate if tool calls and arguments are appropriate.\n",
    "    \n",
    "    Recognizes that consolidated tools can accomplish the same goals as multiple specific tools.\n",
    "    NO FALLBACKS - fails loudly if data missing.\n",
    "    \"\"\"\n",
    "    # NO .get() - fail loudly if missing\n",
    "    actual_trajectory = run.outputs[\"trajectory\"]\n",
    "    expected_trajectory = example.outputs[\"trajectory\"]\n",
    "    \n",
    "    instructions = \"\"\"\n",
    "You are evaluating an AI agent's tool usage. Judge if the agent made appropriate tool calls with correct arguments.\n",
    "\n",
    "A good trajectory:\n",
    "- Calls the right tools for the task (or functionally equivalent consolidated tools)\n",
    "- Uses correct and complete arguments\n",
    "- Doesn't call unnecessary tools\n",
    "- Follows a logical sequence\n",
    "\n",
    "Note: A consolidated tool with parameters can be just as good or better than multiple specific tools.\n",
    "For example, get_order_info(include=[\"status\", \"tracking\"]) accomplishes what get_order() + get_tracking() would do.\n",
    "\"\"\"\n",
    "    \n",
    "    user_context = f\"\"\"\n",
    "<expected_trajectory>\n",
    "{expected_trajectory}\n",
    "</expected_trajectory>\n",
    "\n",
    "<actual_trajectory>\n",
    "{actual_trajectory}\n",
    "</actual_trajectory>\n",
    "\n",
    "Are the actual tool calls appropriate for accomplishing the same goal?\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        grade = trajectory_judge_llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": user_context}\n",
    "        ])\n",
    "        return {\n",
    "            \"key\": \"llm_trajectory\",\n",
    "            \"score\": 1.0 if grade[\"is_appropriate\"] else 0.0,\n",
    "            \"comment\": grade[\"reasoning\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Trajectory eval error: {e}\")\n",
    "        return {\"key\": \"llm_trajectory\", \"score\": 0.0, \"comment\": f\"Evaluation failed: {str(e)[:100]}\"}\n",
    "\n",
    "print(\"âœ“ Defined llm_trajectory_evaluator with improved prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined tool_efficiency_evaluator without fallbacks\n"
     ]
    }
   ],
   "source": [
    "# Tool efficiency evaluator\n",
    "def tool_efficiency_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Measure tool call efficiency relative to expected trajectory.\n",
    "    NO FALLBACKS - fail loudly if data missing.\n",
    "    \n",
    "    Why it matters: Context confusion causes excessive tool calls.\n",
    "    Consolidated tools should be more efficient (fewer calls, same information).\n",
    "    \"\"\"\n",
    "    # NO .get() - fail loudly if missing\n",
    "    actual_trajectory = run.outputs[\"trajectory\"]\n",
    "    expected_trajectory = example.outputs[\"trajectory\"]\n",
    "    \n",
    "    actual_count = len(actual_trajectory)\n",
    "    expected_count = len(expected_trajectory)\n",
    "    \n",
    "    if actual_count == 0:\n",
    "        return {\"key\": \"tool_efficiency\", \"score\": 0.0, \"comment\": \"No tools called\"}\n",
    "    \n",
    "    # Efficiency: perfect if we call exactly expected number or fewer\n",
    "    # Penalize calling MORE tools than needed\n",
    "    efficiency = min(1.0, expected_count / actual_count)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"tool_efficiency\",\n",
    "        \"score\": efficiency,\n",
    "        \"comment\": f\"Called {actual_count} tools (expected {expected_count}). Efficiency: {efficiency:.2f}\"\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Defined tool_efficiency_evaluator without fallbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined success_criteria_evaluator\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Success criteria evaluator\n",
    "class SuccessCriteriaAssessment(TypedDict):\n",
    "    \"\"\"Evaluate if response meets success criteria.\"\"\"\n",
    "    reasoning: str\n",
    "    meets_criteria: bool\n",
    "    score: float  # 0.0 to 1.0\n",
    "\n",
    "def success_criteria_evaluator(run: Run, example: Example) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM-as-judge: STRICT evaluation focused on response completeness and accuracy.\n",
    "    \n",
    "    The consolidated agent should score HIGHER because it provides more complete,\n",
    "    accurate responses by efficiently gathering all needed information.\n",
    "    \"\"\"\n",
    "    final_response = run.outputs[\"final_response\"]\n",
    "    success_criteria = example.outputs[\"success_criteria\"]\n",
    "    \n",
    "    # If no response, return 0\n",
    "    if not final_response or final_response.strip() == \"\":\n",
    "        return {\n",
    "            \"key\": \"success_criteria\",\n",
    "            \"score\": 0.0,\n",
    "            \"comment\": \"No response generated\"\n",
    "        }\n",
    "    \n",
    "    instructions = \"\"\"\n",
    "You are evaluating an AI agent's response against specific success criteria.\n",
    "\n",
    "BE EXTREMELY STRICT on completeness and specificity:\n",
    "- Each criterion must be FULLY satisfied with SPECIFIC details for credit\n",
    "- Vague statements like \"in transit\" WITHOUT a specific location = FAIL (0.0-0.3)\n",
    "- Generic status WITHOUT specific tracking details = FAIL (0.0-0.3)\n",
    "- Missing ANY required criterion significantly reduces the score\n",
    "- Accuracy matters - incorrect or incomplete details should be heavily penalized\n",
    "\n",
    "CRITICAL: For location queries, \"in transit\" is NOT sufficient - need specific city/facility.\n",
    "\n",
    "Scoring rubric:\n",
    "- 1.0: ALL criteria fully satisfied with SPECIFIC, detailed, accurate information\n",
    "- 0.7-0.9: Most criteria satisfied with specific details, very minor incompleteness\n",
    "- 0.4-0.6: Some criteria satisfied but lacks specificity or misses key details\n",
    "- 0.1-0.3: Vague/generic information without specifics (\"in transit\" without location)\n",
    "- 0.0: Completely fails to address criteria or provides wrong information\n",
    "\n",
    "Focus on: Is the response SPECIFIC and COMPLETE? Does it provide EXACT details for every criterion?\n",
    "Demand precision - vague responses should score 0.3 or lower.\n",
    "\"\"\"\n",
    "    \n",
    "    user_context = f\"\"\"\n",
    "<success_criteria>\n",
    "{success_criteria}\n",
    "</success_criteria>\n",
    "\n",
    "<response>\n",
    "{final_response}\n",
    "</response>\n",
    "\n",
    "Evaluate strictly: Does this response FULLY and ACCURATELY meet ALL the success criteria?\n",
    "Check each criterion individually. Be demanding about completeness.\n",
    "\"\"\"\n",
    "    \n",
    "    # Use GPT-4o for better evaluation\n",
    "    judge_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    grader = judge_llm.with_structured_output(SuccessCriteriaAssessment, method=\"function_calling\")\n",
    "    \n",
    "    try:\n",
    "        assessment = grader.invoke([\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": user_context}\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"success_criteria\",\n",
    "            \"score\": assessment[\"score\"],\n",
    "            \"comment\": f\"{assessment['reasoning']}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"success_criteria\",\n",
    "            \"score\": 0.0,\n",
    "            \"comment\": f\"Evaluation error: {str(e)[:150]}\"\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Defined success_criteria_evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 4 evaluators\n",
      "   - trajectory_match: Flexible comparison with mode support\n",
      "   - success_criteria: Boolean scoring for semantic correctness\n",
      "   - llm_trajectory: Recognizes consolidated tool equivalence\n",
      "   - tool_efficiency: Measures call count efficiency\n"
     ]
    }
   ],
   "source": [
    "ALL_EVALUATORS = [\n",
    "    trajectory_match_evaluator,\n",
    "    success_criteria_evaluator,\n",
    "    llm_trajectory_evaluator,\n",
    "    tool_efficiency_evaluator\n",
    "]\n",
    "print(f\"âœ“ Loaded {len(ALL_EVALUATORS)} evaluators\")\n",
    "print(\"   - trajectory_match: Flexible comparison with mode support\")\n",
    "print(\"   - success_criteria: Boolean scoring for semantic correctness\")\n",
    "print(\"   - llm_trajectory: Recognizes consolidated tool equivalence\")\n",
    "print(\"   - tool_efficiency: Measures call count efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Wrapper for Structured Output\n",
    "\n",
    "To enable fair comparisons in our evaluators, we need structured output from the agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created agent wrapper for structured output (no fallbacks)\n"
     ]
    }
   ],
   "source": [
    "def run_agent_with_trajectory(agent, query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Runs an agent and returns structured output.\n",
    "    NO FALLBACKS - fails loudly if expected structure is missing.\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"final_response\": str,  # The final AI message text\n",
    "            \"trajectory\": [         # List of tool calls made\n",
    "                {\"name\": str, \"args\": dict},\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Execute the agent\n",
    "    result = agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "    \n",
    "    # Extract final AI message\n",
    "    final_response = \"\"\n",
    "    trajectory = []\n",
    "    \n",
    "    # NO .get() - fail if messages missing\n",
    "    messages = result[\"messages\"]\n",
    "    \n",
    "    # Extract trajectory from all AI messages with tool calls\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, dict) and msg[\"type\"] == \"ai\":\n",
    "            # Collect tool calls\n",
    "            if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n",
    "                for tc in msg[\"tool_calls\"]:\n",
    "                    trajectory.append({\n",
    "                        \"name\": tc[\"name\"],  # No .get()\n",
    "                        \"args\": tc[\"args\"]   # No .get()\n",
    "                    })\n",
    "            # Get final response (last AI message with content)\n",
    "            if msg[\"content\"]:\n",
    "                final_response = msg[\"content\"]\n",
    "        # Handle AIMessage objects\n",
    "        elif hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tc in msg.tool_calls:\n",
    "                trajectory.append({\n",
    "                    \"name\": tc.name if hasattr(tc, 'name') else tc[\"name\"],\n",
    "                    \"args\": tc.args if hasattr(tc, 'args') else tc[\"args\"]\n",
    "                })\n",
    "        if hasattr(msg, 'content') and msg.content:\n",
    "            final_response = msg.content\n",
    "    \n",
    "    return {\n",
    "        \"final_response\": final_response,\n",
    "        \"trajectory\": trajectory\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Created agent wrapper for structured output (no fallbacks)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Defined get_metrics_from_experiment (no fallbacks, updated for success_criteria)\n"
     ]
    }
   ],
   "source": [
    "# Extract metrics from local experiment results (no API calls needed)\n",
    "def get_metrics_from_experiment(experiment) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract average metrics from local experiment results.\n",
    "    NO FALLBACKS - fails loudly if data missing.\n",
    "    \"\"\"\n",
    "    results = list(experiment)\n",
    "    metrics = {\"trajectory_match\": [], \"llm_trajectory\": [], \"success_criteria\": [], \"tool_efficiency\": []}\n",
    "    \n",
    "    for result in results:\n",
    "        eval_results = result[\"evaluation_results\"][\"results\"]  # No .get()\n",
    "        for eval_result in eval_results:\n",
    "            key = eval_result.key  # No .get()\n",
    "            if key in metrics and eval_result.score is not None:\n",
    "                metrics[key].append(eval_result.score)\n",
    "    \n",
    "    # Calculate averages - fail if no scores found\n",
    "    avg_metrics = {}\n",
    "    for key, scores in metrics.items():\n",
    "        if scores:\n",
    "            avg_metrics[key] = sum(scores) / len(scores)\n",
    "        else:\n",
    "            avg_metrics[key] = 0.0\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "print(\"âœ“ Defined get_metrics_from_experiment (no fallbacks, updated for success_criteria)\")\n",
    "\n",
    "# Experiments and visualizations will be added below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Tool Scaling Degrades Performance\n",
    "\n",
    "**Hypothesis:** The sheer number of tools is overwhelming the model.\n",
    "\n",
    "We've seen the production agent struggle. Now let's prove the relationship between tool count and performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the Problem with LangSmith\n",
    "\n",
    "Let's evaluate our production agent that has access to 75 tools to see what's actually happening:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¬ Running Experiment 1: Production agent with 75 tools...\n",
      "   Expected: LOW performance due to duplicate tools causing confusion\n",
      "View the evaluation results for experiment: 'production-75-tools-3f852cf2' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/46929a17-406b-4431-9eae-cd895dd2265c/compare?selectedSessions=e22306ac-b545-44b9-b372-8bb9dbc60404\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90df7d273efe40ebb5f208c4c6bd80eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Production evaluation complete!\n",
      "   Trajectory Match: 31.25%\n",
      "   Success Criteria: 56.00%\n",
      "   LLM Trajectory: 20.00%\n",
      "   Tool Efficiency: 0.78\n",
      "\n",
      "   ðŸ’¡ Context confusion: Too many similar tools lead to poor tool selection\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 1: Production Reality (~75 tools) - Context Confusion\n",
    "# ============================================================================\n",
    "print(f\"\\nðŸ”¬ Running Experiment 1: Production agent with {len(all_tools)} tools...\")\n",
    "print(\"   Expected: LOW performance due to duplicate tools causing confusion\")\n",
    "\n",
    "production_experiment = evaluate(\n",
    "    lambda inputs: run_agent_with_trajectory(production_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"production-75-tools\",\n",
    "    metadata={\"tool_count\": len(all_tools), \"config\": \"production-overload\"},\n",
    ")\n",
    "production_metrics = get_metrics_from_experiment(production_experiment)\n",
    "print(f\"\\nâœ“ Production evaluation complete!\")\n",
    "print(f\"   Trajectory Match: {production_metrics['trajectory_match']:.2%}\")\n",
    "print(f\"   Success Criteria: {production_metrics['success_criteria']:.2%}\")\n",
    "print(f\"   LLM Trajectory: {production_metrics['llm_trajectory']:.2%}\")\n",
    "print(f\"   Tool Efficiency: {production_metrics['tool_efficiency']:.2f}\")\n",
    "print(\"\\n   ðŸ’¡ Context confusion: Too many similar tools lead to poor tool selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Visualize Real Metrics from LangSmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What LangSmith reveals:**\n",
    "- Agent calls fraud detection tools on simple order lookups\n",
    "- Analytics and reporting tools invoked unnecessarily\n",
    "- More tool calls = longer response times\n",
    "- Responses include irrelevant information\n",
    "\n",
    "This is context confusion in action. Let's understand why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¬ Running Experiment 2: Minimal agent with 6 specific tools...\n",
      "   Expected: MODERATE performance - efficient but missing capabilities\n",
      "View the evaluation results for experiment: 'minimal-6-tools-d567a861' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/46929a17-406b-4431-9eae-cd895dd2265c/compare?selectedSessions=94cd6269-c8b8-4f37-adc0-88dae7d54c14\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54823948f1384a01af5de8dde8a6e510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   Expected: MODERATE performance - efficient but missing capabilities\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m minimal_agent = create_agent(\n\u001b[32m      8\u001b[39m     model=llm,\n\u001b[32m      9\u001b[39m     tools=shipping_core_tools,\n\u001b[32m     10\u001b[39m     system_prompt=SHIPPING_SUPPORT_INSTRUCTIONS\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m minimal_experiment = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_agent_with_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminimal_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mALL_EVALUATORS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mminimal-6-tools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mover-correction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m minimal_metrics = get_metrics_from_experiment(minimal_experiment)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Minimal evaluation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:389\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m     _warn_once(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mupload_results\u001b[39m\u001b[33m'\u001b[39m\u001b[33m parameter is in beta.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    388\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning evaluation over target system \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mEVALUATOR_T\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_repetitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1066\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling)\u001b[39m\n\u001b[32m   1064\u001b[39m     manager = manager.with_summary_evaluators(summary_evaluators)\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Start consuming the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m results = \u001b[43mExperimentResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:549\u001b[39m, in \u001b[36mExperimentResults.__init__\u001b[39m\u001b[34m(self, experiment_manager, blocking)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m._thread = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:574\u001b[39m, in \u001b[36mExperimentResults._process_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m tqdm = _load_tqdm()\n\u001b[32m    573\u001b[39m results = \u001b[38;5;28mself\u001b[39m._manager.get_results()\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1487\u001b[39m, in \u001b[36m_ExperimentManager.get_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterable[ExperimentResultRow]:\n\u001b[32m   1486\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the traces, evaluation results, and associated examples.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_results\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mExperimentResultRow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1467\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Split the generator into three so the manager\u001b[39;00m\n\u001b[32m   1463\u001b[39m \u001b[38;5;66;03m# can consume each value individually.\u001b[39;00m\n\u001b[32m   1464\u001b[39m r1, r2, r3 = itertools.tee(experiment_results, \u001b[32m3\u001b[39m)\n\u001b[32m   1465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copy(\n\u001b[32m   1466\u001b[39m     (result[\u001b[33m\"\u001b[39m\u001b[33mexample\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m r1),\n\u001b[32m-> \u001b[39m\u001b[32m1467\u001b[39m     runs=\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1468\u001b[39m     evaluation_results=(result[\u001b[33m\"\u001b[39m\u001b[33mevaluation_results\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m r3),\n\u001b[32m   1469\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1656\u001b[39m, in \u001b[36m_ExperimentManager._score\u001b[39m\u001b[34m(self, evaluators, max_concurrency)\u001b[39m\n\u001b[32m   1654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_concurrency == \u001b[32m0\u001b[39m:\n\u001b[32m   1655\u001b[39m     context = copy_context()\n\u001b[32m-> \u001b[39m\u001b[32m1656\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcurrent_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcurrent_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1662\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1487\u001b[39m, in \u001b[36m_ExperimentManager.get_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterable[ExperimentResultRow]:\n\u001b[32m   1486\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the traces, evaluation results, and associated examples.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluation_results\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mExperimentResultRow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1442\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1434\u001b[39m _experiment_results = context.run(\n\u001b[32m   1435\u001b[39m     \u001b[38;5;28mself\u001b[39m._predict,\n\u001b[32m   1436\u001b[39m     target,\n\u001b[32m   1437\u001b[39m     max_concurrency=max_concurrency,\n\u001b[32m   1438\u001b[39m     include_attachments=_target_include_attachments(target),\n\u001b[32m   1439\u001b[39m )\n\u001b[32m   1440\u001b[39m r1, r2 = itertools.tee(_experiment_results, \u001b[32m2\u001b[39m)\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copy(\n\u001b[32m-> \u001b[39m\u001b[32m1442\u001b[39m     (pred[\u001b[33m\"\u001b[39m\u001b[33mexample\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m r1), runs=\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1443\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1523\u001b[39m, in \u001b[36m_ExperimentManager._predict\u001b[39m\u001b[34m(self, target, max_concurrency, include_attachments)\u001b[39m\n\u001b[32m   1521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_concurrency == \u001b[32m0\u001b[39m:\n\u001b[32m   1522\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.examples:\n\u001b[32m-> \u001b[39m\u001b[32m1523\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_upload_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m            \u001b[49m\u001b[43minclude_attachments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1535\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ls_utils.ContextThreadPoolExecutor(max_concurrency) \u001b[38;5;28;01mas\u001b[39;00m executor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:1887\u001b[39m, in \u001b[36m_forward\u001b[39m\u001b[34m(fn, example, experiment_name, metadata, client, upload_results, include_attachments, error_handling)\u001b[39m\n\u001b[32m   1885\u001b[39m arg_names = _get_target_args(fn)\n\u001b[32m   1886\u001b[39m args = [\u001b[38;5;28mgetattr\u001b[39m(example, argn) \u001b[38;5;28;01mfor\u001b[39;00m argn \u001b[38;5;129;01min\u001b[39;00m arg_names]\n\u001b[32m-> \u001b[39m\u001b[32m1887\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlangsmith_extra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlangsmith_extra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[38;5;66;03m# Reset attachment readers if attachments were used.\u001b[39;00m\n\u001b[32m   1889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_attachments \u001b[38;5;129;01mand\u001b[39;00m example.attachments \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   Expected: MODERATE performance - efficient but missing capabilities\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m minimal_agent = create_agent(\n\u001b[32m      8\u001b[39m     model=llm,\n\u001b[32m      9\u001b[39m     tools=shipping_core_tools,\n\u001b[32m     10\u001b[39m     system_prompt=SHIPPING_SUPPORT_INSTRUCTIONS\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m minimal_experiment = evaluate(\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m inputs: \u001b[43mrun_agent_with_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminimal_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     15\u001b[39m     data=dataset_name,\n\u001b[32m     16\u001b[39m     evaluators=ALL_EVALUATORS,\n\u001b[32m     17\u001b[39m     experiment_prefix=\u001b[33m\"\u001b[39m\u001b[33mminimal-6-tools\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     metadata={\u001b[33m\"\u001b[39m\u001b[33mtool_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m6\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mover-correction\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m minimal_metrics = get_metrics_from_experiment(minimal_experiment)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Minimal evaluation complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mrun_agent_with_trajectory\u001b[39m\u001b[34m(agent, query)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mRuns an agent and returns structured output.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mNO FALLBACKS - fails loudly if expected structure is missing.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m    }\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Execute the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Extract final AI message\u001b[39;00m\n\u001b[32m     19\u001b[39m final_response = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain/agents/factory.py:1131\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1118\u001b[39m request = ModelRequest(\n\u001b[32m   1119\u001b[39m     model=model,\n\u001b[32m   1120\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1126\u001b[39m     runtime=runtime,\n\u001b[32m   1127\u001b[39m )\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1130\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1133\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1134\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain/agents/factory.py:1102\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1100\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1104\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5548\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5541\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5543\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5546\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5547\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5549\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5550\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5551\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_anthropic/chat_models.py:1319\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1317\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1321\u001b[39m     _handle_anthropic_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/langchain_anthropic/chat_models.py:1175\u001b[39m, in \u001b[36mChatAnthropic._create\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n\u001b[32m   1174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.beta.messages.create(**payload)\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/anthropic/_utils/_utils.py:282\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/anthropic/resources/messages/messages.py:930\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    924\u001b[39m     warnings.warn(\n\u001b[32m    925\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    926\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    927\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    928\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/anthropic/_base_client.py:1326\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1313\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1314\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1321\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1322\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1323\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1324\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1325\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/anthropic/_base_client.py:1049\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1047\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1055\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/demos/context-failure-evals/.venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.9-macos-aarch64-none/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.9-macos-aarch64-none/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 2: Over-Correction (6 specific tools) - Limited Capability\n",
    "# ============================================================================\n",
    "print(f\"\\nðŸ”¬ Running Experiment 2: Minimal agent with 6 specific tools...\")\n",
    "print(\"   Expected: MODERATE performance - efficient but missing capabilities\")\n",
    "\n",
    "minimal_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=shipping_core_tools,\n",
    "    system_prompt=SHIPPING_SUPPORT_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "minimal_experiment = evaluate(\n",
    "    lambda inputs: run_agent_with_trajectory(minimal_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"minimal-6-tools\",\n",
    "    metadata={\"tool_count\": 6, \"config\": \"over-correction\"},\n",
    ")\n",
    "\n",
    "minimal_metrics = get_metrics_from_experiment(minimal_experiment)\n",
    "print(f\"\\nâœ“ Minimal evaluation complete!\")\n",
    "print(f\"   Trajectory Match: {minimal_metrics['trajectory_match']:.2%}\")\n",
    "print(f\"   Success Criteria: {minimal_metrics['success_criteria']:.2%}\")\n",
    "print(f\"   LLM Trajectory: {minimal_metrics['llm_trajectory']:.2%}\")\n",
    "print(f\"   Tool Efficiency: {minimal_metrics['tool_efficiency']:.2f}\")\n",
    "print(\"\\n   ðŸ’¡ Overcorrection: Too few tools miss capabilities (returns, carrier incidents, preferences)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 1: Tool Consolidation\n",
    "\n",
    "**Insight:** Flexible tools with parameters beat many specific tools.\n",
    "\n",
    "Instead of 75 separate separate tools, create 12 more flexible tools with parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 12 consolidated tools\n",
      "   These 12 tools replace ~20 commonly used specific tools\n"
     ]
    }
   ],
   "source": [
    "from context_confusion.resources.mock_orders import ORDERS, SHIPMENTS, ORDER_EVENTS\n",
    "from context_confusion.resources.mock_customers import CUSTOMERS\n",
    "from context_confusion.resources.mock_carriers import TRACKING_SCANS\n",
    "from typing import Optional\n",
    "\n",
    "def get_order_info(\n",
    "    order_id: str,\n",
    "    include: List[Literal[\"status\", \"tracking\", \"events\", \"shipment\", \"customer\"]] = [\"status\", \"shipment\"]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve comprehensive order information with flexible parameters.\n",
    "    \n",
    "    This consolidated tool replaces multiple specific tools:\n",
    "    - get_order (basic status)\n",
    "    - get_shipment (carrier & delivery info)\n",
    "    - get_order_events (history & tracking events)\n",
    "    - get_order_summary (combined view)\n",
    "    - check_order_status (status lookup)\n",
    "    \n",
    "    Design advantage: Single flexible call with 'include' parameter replaces 5+ separate tool calls.\n",
    "    Reduces context confusion by providing one clear choice for order queries.\n",
    "    \n",
    "    Args:\n",
    "        order_id: Order ID to look up  \n",
    "        include: Information types to fetch in ONE call:\n",
    "                - \"status\": Current order state & timestamps\n",
    "                - \"tracking\": Tracking number & scan history\n",
    "                - \"events\": Full order event log\n",
    "                - \"shipment\": Carrier, service level, ETA\n",
    "                - \"customer\": Associated customer details\n",
    "    \n",
    "    Example:\n",
    "        get_order_info(\"12345\", include=[\"status\", \"tracking\", \"shipment\"])\n",
    "        Returns complete info in one call instead of 3 separate calls\n",
    "    \"\"\"\n",
    "    oid = order_id.strip().lstrip(\"#\")\n",
    "    \n",
    "    if oid not in ORDERS:\n",
    "        return {\"ok\": False, \"error\": f\"Order not found: {oid}\"}\n",
    "    \n",
    "    order = ORDERS[oid]\n",
    "    result = {\"ok\": True, \"order_id\": oid}\n",
    "    \n",
    "    if \"status\" in include:\n",
    "        result.update({\n",
    "            \"status\": order[\"status\"],\n",
    "            \"order_date\": order[\"order_date\"],\n",
    "            \"last_update\": order[\"last_update\"],\n",
    "            \"total\": {\"cents\": order[\"total_cents\"], \"currency\": order[\"currency\"]}\n",
    "        })\n",
    "    \n",
    "    if \"tracking\" in include and order[\"tracking_number\"]:\n",
    "        tracking_num = order[\"tracking_number\"]\n",
    "        result[\"tracking_number\"] = tracking_num\n",
    "        if tracking_num in TRACKING_SCANS:\n",
    "            result[\"tracking_scans\"] = TRACKING_SCANS[tracking_num]\n",
    "    \n",
    "    if \"events\" in include and oid in ORDER_EVENTS:\n",
    "        result[\"events\"] = ORDER_EVENTS[oid]\n",
    "    \n",
    "    if \"shipment\" in include and oid in SHIPMENTS:\n",
    "        shipment = SHIPMENTS[oid]\n",
    "        result[\"shipment\"] = {\n",
    "            \"carrier\": shipment[\"carrier\"],\n",
    "            \"service_level\": shipment[\"service_level\"],\n",
    "            \"eta_date\": shipment[\"eta_date\"],\n",
    "            \"latest_scan\": shipment[\"latest_scan\"],\n",
    "            \"scan_location\": shipment[\"scan_location\"],\n",
    "            \"scan_timestamp\": shipment[\"scan_timestamp\"],\n",
    "        }\n",
    "    \n",
    "    if \"customer\" in include:\n",
    "        customer = CUSTOMERS.get(order[\"customer_id\"], {})\n",
    "        result[\"customer\"] = {\n",
    "            \"email\": customer.get(\"email\"),\n",
    "            \"name\": customer.get(\"name\"),\n",
    "            \"tier\": customer.get(\"tier\"),\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "from context_confusion.tools import (\n",
    "    get_customer_by_email, get_customer, get_customer_preferences, get_billing_info,\n",
    "    get_tracking_details, get_carrier_info as get_carrier_info_orig, get_carrier_incidents,\n",
    "    get_shipping_rates, get_carrier_performance, get_return_request, create_return_label,\n",
    "    cancel_order, hold_order, expedite_order, update_delivery_address, process_refund,\n",
    "    get_warehouse_info as get_warehouse_info_orig, check_inventory, get_warehouse_incidents,\n",
    "    create_support_ticket, send_notification, apply_credit, check_fraud_score\n",
    ")\n",
    "from context_confusion.resources.mock_customers import CUSTOMERS, CUSTOMER_EMAIL_MAP, CUSTOMER_PREFERENCES, BILLING_INFO\n",
    "from context_confusion.resources.mock_warehouses import WAREHOUSES, INVENTORY, WAREHOUSE_INCIDENTS\n",
    "from context_confusion.resources.mock_carriers import CARRIERS, CARRIER_INCIDENTS, RATE_CARDS\n",
    "\n",
    "# ============================================================================\n",
    "# SWEET SPOT: 12 Consolidated Tools with Flexible Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# Tool 1: get_order_info (already defined above)\n",
    "\n",
    "# Tool 2: get_customer_info\n",
    "def get_customer_info(\n",
    "    identifier: str,\n",
    "    lookup_by: Literal[\"email\", \"customer_id\"] = \"email\",\n",
    "    include: List[Literal[\"profile\", \"preferences\", \"billing\"]] = [\"profile\"]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve customer information with flexible lookup and detail levels.\n",
    "    \n",
    "    This consolidated tool replaces 4 separate customer lookup tools:\n",
    "    - get_customer (lookup by ID)\n",
    "    - get_customer_by_email (lookup by email)\n",
    "    - get_customer_preferences (delivery preferences)\n",
    "    - get_billing_info (payment & billing details)\n",
    "    \n",
    "    Design advantage: Single tool handles both lookup methods AND multiple info types.\n",
    "    Agent doesn't need to choose between \"get_customer\" vs \"get_customer_by_email\" - \n",
    "    just specify lookup_by parameter. Reduces decision fatigue and tool confusion.\n",
    "    \n",
    "    Args:\n",
    "        identifier: Customer email or ID\n",
    "        lookup_by: How to find customer - \"email\" or \"customer_id\"\n",
    "        include: Information types to retrieve:\n",
    "                - \"profile\": Name, email, tier, phone, location\n",
    "                - \"preferences\": Delivery instructions, notifications\n",
    "                - \"billing\": Payment methods, billing address\n",
    "    \n",
    "    Example:\n",
    "        get_customer_info(\"user@example.com\", lookup_by=\"email\", include=[\"profile\", \"preferences\"])\n",
    "        Replaces: get_customer_by_email() + get_customer_preferences() (2 calls â†’ 1)\n",
    "    \"\"\"\n",
    "    if lookup_by == \"email\":\n",
    "        email = identifier.strip().lower()\n",
    "        if email not in CUSTOMER_EMAIL_MAP:\n",
    "            return {\"ok\": False, \"error\": \"Customer not found\"}\n",
    "        cid = CUSTOMER_EMAIL_MAP[email]\n",
    "    else:\n",
    "        cid = identifier.strip()\n",
    "        if cid not in CUSTOMERS:\n",
    "            return {\"ok\": False, \"error\": \"Customer not found\"}\n",
    "    \n",
    "    customer = CUSTOMERS[cid]\n",
    "    result = {\"ok\": True, \"customer_id\": cid}\n",
    "    \n",
    "    if \"profile\" in include:\n",
    "        result[\"profile\"] = {\n",
    "            \"email\": customer[\"email\"],\n",
    "            \"name\": customer[\"name\"],\n",
    "            \"tier\": customer[\"tier\"],\n",
    "            \"phone\": customer[\"phone\"],\n",
    "            \"location\": customer[\"location\"]\n",
    "        }\n",
    "    \n",
    "    if \"preferences\" in include and cid in CUSTOMER_PREFERENCES:\n",
    "        result[\"preferences\"] = CUSTOMER_PREFERENCES[cid]\n",
    "    \n",
    "    if \"billing\" in include and cid in BILLING_INFO:\n",
    "        result[\"billing\"] = BILLING_INFO[cid]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tool 3: get_tracking_info\n",
    "def get_tracking_info(\n",
    "    tracking_number: str,\n",
    "    include: List[Literal[\"scans\", \"events\", \"eta\"]] = [\"scans\"]\n",
    ") -> dict:\n",
    "    \"\"\"Retrieve detailed tracking information. Replaces: get_tracking_details (keeps but enhances)\"\"\"\n",
    "    return get_tracking_details(tracking_number)\n",
    "\n",
    "# Tool 4: get_carrier_info\n",
    "def get_carrier_info(\n",
    "    carrier_id: Optional[str] = None,\n",
    "    include: List[Literal[\"details\", \"incidents\", \"rates\", \"performance\"]] = [\"details\"],\n",
    "    date: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve carrier information including incidents, rates, and performance.\n",
    "    \n",
    "    This consolidated tool replaces 4 carrier-related tools:\n",
    "    - get_carrier_info (basic carrier details)\n",
    "    - get_carrier_incidents (service disruptions)\n",
    "    - get_shipping_rates (cost estimation)\n",
    "    - get_carrier_performance (metrics & SLA)\n",
    "    \n",
    "    Design advantage: All carrier queries go through ONE tool with flexible parameters.\n",
    "    Critical for diagnosing delivery issues where you need both carrier details AND incidents.\n",
    "    \n",
    "    Args:\n",
    "        carrier_id: Carrier to look up (optional for incident checks)\n",
    "        include: Information types:\n",
    "                - \"details\": Carrier name, services, contact\n",
    "                - \"incidents\": Service disruptions & delays\n",
    "                - \"rates\": Shipping cost information\n",
    "                - \"performance\": On-time delivery metrics\n",
    "        date: Required for incident checks (YYYY-MM-DD format)\n",
    "    \n",
    "    Example:\n",
    "        get_carrier_info(\"ups\", include=[\"details\", \"incidents\"], date=\"2024-12-20\")\n",
    "        One call replaces: get_carrier_info(\"ups\") + get_carrier_incidents(\"2024-12-20\")\n",
    "    \"\"\"\n",
    "    result = {\"ok\": True}\n",
    "    \n",
    "    if carrier_id and \"details\" in include:\n",
    "        carrier_result = get_carrier_info_orig(carrier_id)\n",
    "        if carrier_result[\"ok\"]:\n",
    "            result[\"carrier\"] = carrier_result[\"data\"]\n",
    "    \n",
    "    if \"incidents\" in include and date:\n",
    "        incidents_result = get_carrier_incidents(date)\n",
    "        if incidents_result[\"ok\"]:\n",
    "            result[\"incidents\"] = incidents_result[\"data\"]\n",
    "    \n",
    "    if \"rates\" in include:\n",
    "        result[\"rate_info\"] = \"Use get_shipping_rates with origin/destination\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tool 5: get_return_info\n",
    "def get_return_info(\n",
    "    order_id: str,\n",
    "    include: List[Literal[\"request\", \"label\", \"status\"]] = [\"request\"]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve or manage return information.\n",
    "    \n",
    "    This consolidated tool replaces 2 return-related tools:\n",
    "    - get_return_request (check return status)\n",
    "    - create_return_label (generate label)\n",
    "    \n",
    "    Design advantage: All return queries through one tool. Agent doesn't need to decide\n",
    "    between multiple return tools - just specify what info is needed via 'include'.\n",
    "    \n",
    "    Args:\n",
    "        order_id: Order to check returns for\n",
    "        include: Information types:\n",
    "                - \"request\": Return request status\n",
    "                - \"label\": Return shipping label\n",
    "                - \"status\": Current return state\n",
    "    \"\"\"\n",
    "    return get_return_request(order_id)\n",
    "\n",
    "# Tool 6: manage_order\n",
    "def manage_order(\n",
    "    order_id: str,\n",
    "    action: Literal[\"cancel\", \"hold\", \"expedite\", \"update_address\"],\n",
    "    **params\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Perform order management actions.\n",
    "    \n",
    "    This consolidated tool replaces 4 order modification tools:\n",
    "    - cancel_order (cancel processing)\n",
    "    - hold_order (pause fulfillment)\n",
    "    - expedite_order (upgrade shipping speed)\n",
    "    - update_delivery_address (change destination)\n",
    "    \n",
    "    Design advantage: Single \"manage_order\" tool with 'action' parameter replaces 4 tools.\n",
    "    Agent chooses the action, not the tool - clearer intent, less confusion.\n",
    "    Extensible design allows adding new actions without new tools.\n",
    "    \n",
    "    Args:\n",
    "        order_id: Order to modify\n",
    "        action: Management action to perform\n",
    "        **params: Action-specific parameters (reason, new_address, etc)\n",
    "    \n",
    "    Example:\n",
    "        manage_order(\"12345\", action=\"expedite\", new_shipping_method=\"express\")\n",
    "        Replaces: expedite_order(\"12345\", \"express\")\n",
    "    \"\"\"\n",
    "    if action == \"cancel\":\n",
    "        return cancel_order(order_id, params.get(\"reason\", \"Customer request\"))\n",
    "    elif action == \"hold\":\n",
    "        return hold_order(order_id, params.get(\"reason\", \"Hold requested\"))\n",
    "    elif action == \"expedite\":\n",
    "        return expedite_order(order_id, params.get(\"new_shipping_method\", \"express\"))\n",
    "    elif action == \"update_address\":\n",
    "        return update_delivery_address(order_id, params.get(\"new_address\", {}))\n",
    "    return {\"ok\": False, \"error\": \"Invalid action\"}\n",
    "\n",
    "# Tool 7-12: Keep specific action tools as-is\n",
    "# process_refund, get_warehouse_info_consolidated, create_support_ticket, \n",
    "# send_notification, apply_credit, check_fraud_score\n",
    "\n",
    "def get_warehouse_info_consolidated(\n",
    "    include: List[Literal[\"locations\", \"inventory\", \"incidents\"]] = [\"locations\"],\n",
    "    sku: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve warehouse and inventory information.\n",
    "    \n",
    "    This consolidated tool replaces 3 warehouse tools:\n",
    "    - get_warehouse_info (facility locations)\n",
    "    - check_inventory (stock levels)\n",
    "    - get_warehouse_incidents (facility issues)\n",
    "    \n",
    "    Design advantage: All warehouse queries through one flexible tool.\n",
    "    Particularly useful for stock availability checks where you might need\n",
    "    both inventory levels AND warehouse incidents affecting fulfillment.\n",
    "    \n",
    "    Args:\n",
    "        include: Information types:\n",
    "                - \"locations\": Warehouse facilities\n",
    "                - \"inventory\": Stock levels (all or by SKU)\n",
    "                - \"incidents\": Facility disruptions\n",
    "        sku: Optional SKU to filter inventory check\n",
    "    \n",
    "    Example:\n",
    "        get_warehouse_info_consolidated(include=[\"inventory\", \"incidents\"], sku=\"WIDGET-123\")\n",
    "        Replaces: check_inventory(\"WIDGET-123\") + get_warehouse_incidents()\n",
    "    \"\"\"\n",
    "    result = {\"ok\": True}\n",
    "    \n",
    "    if \"locations\" in include:\n",
    "        result[\"warehouses\"] = WAREHOUSES\n",
    "    \n",
    "    if \"inventory\" in include:\n",
    "        if sku:\n",
    "            result[\"inventory\"] = {k: v for k, v in INVENTORY.items() if k == sku}\n",
    "        else:\n",
    "            result[\"inventory\"] = INVENTORY\n",
    "    \n",
    "    if \"incidents\" in include:\n",
    "        result[\"incidents\"] = WAREHOUSE_INCIDENTS\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Final consolidated tools list (12 tools)\n",
    "consolidated_tools = [\n",
    "    get_order_info,\n",
    "    get_customer_info,\n",
    "    get_tracking_info,\n",
    "    get_carrier_info,\n",
    "    get_return_info,\n",
    "    manage_order,\n",
    "    process_refund,\n",
    "    get_warehouse_info_consolidated,\n",
    "    create_support_ticket,\n",
    "    send_notification,\n",
    "    apply_credit,\n",
    "    check_fraud_score,\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(consolidated_tools)} consolidated tools\")\n",
    "print(\"   These 12 tools replace ~20 commonly used specific tools\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¬ Running Experiment 3: Optimal agent with 12 consolidated tools...\n",
      "   Expected: HIGH performance - clarity + capability\n",
      "View the evaluation results for experiment: 'optimal-12-consolidated-f1af69ee' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/46929a17-406b-4431-9eae-cd895dd2265c/compare?selectedSessions=c64b81e3-e29e-444e-8108-fa8be551309b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeaa824837f4e74a9ffc872055cbf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Optimal evaluation complete!\n",
      "   Trajectory Match: 0.00%\n",
      "   Success Criteria: 83.00%\n",
      "   LLM Trajectory: 70.00%\n",
      "   Tool Efficiency: 0.90\n",
      "\n",
      "   âœ… Sweet spot: Consolidated tools with parameters provide clarity AND capability!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT 3: 12 consolidated tools - Balance of tool number\n",
    "# ============================================================================\n",
    "from context_confusion.instructions import CONSOLIDATED_INSTRUCTIONS\n",
    "\n",
    "print(f\"\\nðŸ”¬ Running Experiment 3: Optimal agent with 12 consolidated tools...\")\n",
    "print(\"   Expected: HIGH performance - clarity + capability\")\n",
    "\n",
    "optimal_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=consolidated_tools,\n",
    "    system_prompt=CONSOLIDATED_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "optimal_experiment = evaluate(\n",
    "    lambda inputs: run_agent_with_trajectory(optimal_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"optimal-12-consolidated\",\n",
    "    metadata={\"tool_count\": 12, \"config\": \"sweet-spot\"},\n",
    ")\n",
    "\n",
    "optimal_metrics = get_metrics_from_experiment(optimal_experiment)\n",
    "print(f\"\\nâœ“ Optimal evaluation complete!\")\n",
    "print(f\"   Trajectory Match: {optimal_metrics['trajectory_match']:.2%}\")\n",
    "print(f\"   Success Criteria: {optimal_metrics['success_criteria']:.2%}\")\n",
    "print(f\"   LLM Trajectory: {optimal_metrics['llm_trajectory']:.2%}\")\n",
    "print(f\"   Tool Efficiency: {optimal_metrics['tool_efficiency']:.2f}\")\n",
    "print(\"\\n   âœ… Sweet spot: Consolidated tools with parameters provide clarity AND capability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Count Comparison\n",
    "\n",
    "The results reveal a clear U-curve - performance suffers at both extremes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Optimal experiment (Experiment 3 above) already tests consolidated tools\n",
    "# No separate test needed here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics already extracted in Experiment 3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparison visualization showing clear accuracy wins\n",
    "configs = [\"75 Tools\\n(Overload)\", \"6 Tools\\n(Limited)\", \"12 Tools\\n(Consolidated)\"]\n",
    "trajectory_scores = [\n",
    "    production_metrics[\"trajectory_match\"],\n",
    "    minimal_metrics[\"trajectory_match\"],\n",
    "    optimal_metrics[\"trajectory_match\"]\n",
    "]\n",
    "success_scores = [\n",
    "    production_metrics[\"success_criteria\"],\n",
    "    minimal_metrics[\"success_criteria\"],\n",
    "    optimal_metrics[\"success_criteria\"]\n",
    "]\n",
    "efficiency_scores = [\n",
    "    production_metrics[\"tool_efficiency\"],\n",
    "    minimal_metrics[\"tool_efficiency\"],\n",
    "    optimal_metrics[\"tool_efficiency\"]\n",
    "]\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Trajectory Match',\n",
    "    x=configs,\n",
    "    y=trajectory_scores,\n",
    "    marker_color='#1f77b4'\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Success Criteria',\n",
    "    x=configs,\n",
    "    y=success_scores,\n",
    "    marker_color='#2ca02c'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Finding the Sweet Spot: Consolidated Tools Win on Accuracy\",\n",
    "    yaxis_title=\"Score\",\n",
    "    barmode='group',\n",
    "    height=450,\n",
    "    showlegend=True,\n",
    "    yaxis=dict(range=[0, 1.0])\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create summary table\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Configuration\": configs,\n",
    "    \"Tool Count\": [len(all_tools), 6, 12],\n",
    "    \"Trajectory Match\": [f\"{s:.1%}\" for s in trajectory_scores],\n",
    "    \"Success Criteria\": [f\"{s:.1%}\" for s in success_scores],\n",
    "    \"Tool Efficiency\": [f\"{s:.2f}\" for s in efficiency_scores],\n",
    "})\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nðŸ“Š Key Findings:\")\n",
    "print(f\"   75 tools: {trajectory_scores[0]:.1%} trajectory match, {success_scores[0]:.1%} success criteria\")\n",
    "print(f\"   6 tools:  {trajectory_scores[1]:.1%} trajectory match, {success_scores[1]:.1%} success criteria\")\n",
    "print(f\"   12 tools: {trajectory_scores[2]:.1%} trajectory match, {success_scores[2]:.1%} success criteria\")\n",
    "print(\"\\n   âœ… Sweet Spot Achieved:\")\n",
    "print(\"   Consolidated tools outperform both extremes by reducing context confusion\")\n",
    "print(\"   Flexible parameters provide clarity (better tool selection) AND capability (full coverage)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangSmith shows us:** Fewer, focused tools perform better than many scattered tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated tool dataset - same queries, slightly different expected trajectory\n",
    "consolidated_dataset_name = \"shipping-support-queries-consolidated\"\n",
    "\n",
    "test_cases_consolidated = [\n",
    "    {\n",
    "        \"query\": \"I'm user@example.com, what's the status of order #84721?\",\n",
    "        \"final_response\": \"Order #84721 is currently in transit.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_customer_by_email\", \"args\": {\"email\": \"user@example.com\"}},\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"84721\", \"include\": [\"status\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should return order status: IN_TRANSIT\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Track order 99002\",\n",
    "        \"final_response\": \"Order #99002 has been delivered successfully.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"99002\", \"include\": [\"status\", \"shipment\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should show DELIVERED status\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm buyer@uk-shop.co.uk, why is order #23456 delayed?\",\n",
    "        \"final_response\": \"Order #23456 is experiencing a customs delay.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_customer_by_email\", \"args\": {\"email\": \"buyer@uk-shop.co.uk\"}},\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"23456\", \"include\": [\"status\", \"shipment\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should mention customs delay\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm ops@widget.io and I need urgent help with order #10015\",\n",
    "        \"final_response\": \"As a platinum tier customer, I'll prioritize your request for order #10015.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_customer_by_email\", \"args\": {\"email\": \"ops@widget.io\"}},\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"10015\", \"include\": [\"status\", \"customer\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should recognize platinum tier customer\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the tracking number for order 84721?\",\n",
    "        \"final_response\": \"The tracking number for order #84721 is TRK84721ABC.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"84721\", \"include\": [\"tracking\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should return tracking number\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When will order #98765 arrive?\",\n",
    "        \"final_response\": \"Order #98765 is estimated to arrive on March 20, 2024.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"98765\", \"include\": [\"shipment\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should provide ETA date\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I'm manager@euro-corp.eu, can you tell me about order #45678?\",\n",
    "        \"final_response\": \"Order #45678 status available. Please note EU privacy requirements.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_customer_by_email\", \"args\": {\"email\": \"manager@euro-corp.eu\"}},\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"45678\", \"include\": [\"status\", \"customer\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should note EU customer privacy\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Show me the delivery history for order 84721\",\n",
    "        \"final_response\": \"Order #84721 delivery history shows multiple scan events.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"84721\", \"include\": [\"tracking\", \"events\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should show scan history\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What carrier is handling order #23456?\",\n",
    "        \"final_response\": \"Order #23456 is being shipped by Royal Mail.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"23456\", \"include\": [\"shipment\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should return carrier: Royal Mail\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Has order #11111 been delivered?\",\n",
    "        \"final_response\": \"Order #11111 has been returned.\",\n",
    "        \"trajectory\": [\n",
    "            {\"tool\": \"get_order_info\", \"args\": {\"order_id\": \"11111\", \"include\": [\"status\"]}}\n",
    "        ],\n",
    "        \"success_criteria\": \"Should return status: RETURNED\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create consolidated dataset - force recreate to ensure correct structure\n",
    "try:\n",
    "    existing = client.read_dataset(dataset_name=consolidated_dataset_name)\n",
    "    client.delete_dataset(dataset_id=existing.id)\n",
    "    print(f\"âœ“ Deleted old consolidated dataset\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "consolidated_dataset = client.create_dataset(\n",
    "    dataset_name=consolidated_dataset_name,\n",
    "    description=\"Test queries with consolidated tool references\"\n",
    ")\n",
    "for case in test_cases_consolidated:\n",
    "    client.create_example(\n",
    "        inputs={\"query\": case[\"query\"]},\n",
    "        outputs={\n",
    "            \"final_response\": case[\"final_response\"],\n",
    "            \"trajectory\": case[\"trajectory\"],\n",
    "            \"success_criteria\": case[\"success_criteria\"]\n",
    "        },\n",
    "        dataset_id=consolidated_dataset.id\n",
    "    )\n",
    "print(f\"âœ“ Created consolidated dataset with {len(test_cases_consolidated)} examples\")\n",
    "print(f\"   View dataset: https://smith.langchain.com/o/{client._get_tenant_id()}/datasets/{consolidated_dataset.id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2: Irrelevant Tools Cause Distraction\n",
    "\n",
    "**Hypothesis:** It's not just quantity - irrelevant tools are the real problem.\n",
    "\n",
    "Test different ratios of relevant:irrelevant tools (total ~20).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise ratio configurations\n",
    "noise_configs = [\n",
    "    {\"name\": \"100-pct-relevant\", \"tools\": shipping_core_tools + carrier_tools + returns_tools + warehouse_tools[:2]},\n",
    "    {\"name\": \"75-pct-relevant\", \"tools\": shipping_core_tools + carrier_tools + returns_tools + fraud_tools[:3]},\n",
    "    {\"name\": \"50-pct-relevant\", \"tools\": shipping_core_tools + carrier_tools + analytics_tools + marketing_tools[:4]},\n",
    "    {\"name\": \"25-pct-relevant\", \"tools\": shipping_core_tools + fraud_tools + analytics_tools + marketing_tools + vendor_tools[:2]},\n",
    "]\n",
    "\n",
    "print(\"Noise ratio configurations:\")\n",
    "for config in noise_configs:\n",
    "    print(f\"  {config['name']}: {len(config['tools'])} total tools\")\n",
    "\n",
    "# Run experiments\n",
    "for config in noise_configs:\n",
    "    print(f\"\\nTesting {config['name']}...\")\n",
    "    \n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=config[\"tools\"],\n",
    "        system_prompt=SHIPPING_SUPPORT_INSTRUCTIONS\n",
    "    )\n",
    "    \n",
    "    evaluate(\n",
    "        lambda inputs: run_agent_with_trajectory(agent, inputs[\"query\"]),\n",
    "        data=dataset_name,\n",
    "        evaluators=ALL_EVALUATORS,\n",
    "        experiment_prefix=f\"noise-{config['name']}\",\n",
    "        metadata={\"config\": config[\"name\"]},\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ“ Complete\")\n",
    "\n",
    "print(\"\\nâœ“ All noise ratio experiments complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Irrelevant Tools\n",
    "\n",
    "Let's extract and visualize the results to see how noise affects performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics from noise ratio experiments\n",
    "noise_percentages = [100, 75, 50, 25]\n",
    "noise_configs_names = [\"100-pct-relevant\", \"75-pct-relevant\", \"50-pct-relevant\", \"25-pct-relevant\"]\n",
    "\n",
    "relevance_by_noise = []\n",
    "unnecessary_by_noise = []\n",
    "\n",
    "for config_name in noise_configs_names:\n",
    "    exp_name = f\"noise-{config_name}\"\n",
    "    metrics = get_metrics_from_experiment(exp_name)\n",
    "    relevance_by_noise.append(metrics.get(\"trajectory_match\", 0))\n",
    "    unnecessary_by_noise.append(metrics.get(\"correctness\", 0))\n",
    "\n",
    "# Visualize the impact\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=noise_percentages,\n",
    "    y=relevance_by_noise,\n",
    "    mode='lines+markers',\n",
    "    name='Trajectory Match',\n",
    "    line=dict(color='#1f77b4', width=3),\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=noise_percentages,\n",
    "    y=unnecessary_by_noise,\n",
    "    mode='lines+markers',\n",
    "    name='Correctness',\n",
    "    line=dict(color='#2ca02c', width=3),\n",
    "    marker=dict(size=10),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Impact of Irrelevant Tools on Agent Performance\",\n",
    "    xaxis_title=\"% Relevant Tools\",\n",
    "    yaxis_title=\"Trajectory Match Score\",\n",
    "    yaxis2=dict(\n",
    "        title=\"Correctness Score\",\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "    hovermode='x unified',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Create comparison table\n",
    "noise_comparison = {\n",
    "    \"% Relevant\": noise_percentages,\n",
    "    \"Trajectory Match\": relevance_by_noise if relevance_by_noise[0] > 0 else [\"Run eval\"] * 4,\n",
    "    \"Correctness\": unnecessary_by_noise if unnecessary_by_noise[0] > 0 else [\"Run eval\"] * 4,\n",
    "}\n",
    "\n",
    "df_noise = pd.DataFrame(noise_comparison)\n",
    "display(df_noise)\n",
    "\n",
    "print(\"\\nðŸ“Š Key Findings:\")\n",
    "if relevance_by_noise[0] > 0:\n",
    "    print(f\"   100% relevant tools: {relevance_by_noise[0]:.2f} trajectory match, {unnecessary_by_noise[0]:.2f} correctness\")\n",
    "    print(f\"   25% relevant tools: {relevance_by_noise[3]:.2f} trajectory match, {unnecessary_by_noise[3]:.2f} correctness\")\n",
    "    degradation = ((relevance_by_noise[0] - relevance_by_noise[3]) / relevance_by_noise[0] * 100) if relevance_by_noise[0] > 0 else 0\n",
    "    print(f\"   Degradation: {degradation:.0f}% drop in trajectory match\")\n",
    "else:\n",
    "    print(\"   Run evaluations above to see actual metrics\")\n",
    "print(\"   - Even with constant tool count (~20), noise degrades performance\")\n",
    "print(\"   - Irrelevant tools lead to incorrect tool calls\")\n",
    "print(\"   - Quality of tools matters as much as quantity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 2: Smart Tool Filtering with Routing\n",
    "\n",
    "**Insight:** Classify query intent, then provide only relevant tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query router\n",
    "def classify_query_intent(query: str) -> Literal[\"shipping\", \"returns\", \"billing\", \"warehouse\"]:\n",
    "    \"\"\"Classify intent - in production use an LLM.\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    if any(word in query_lower for word in [\"return\", \"refund\", \"send back\"]):\n",
    "        return \"returns\"\n",
    "    elif any(word in query_lower for word in [\"bill\", \"charge\", \"payment\"]):\n",
    "        return \"billing\"\n",
    "    elif any(word in query_lower for word in [\"stock\", \"inventory\", \"warehouse\"]):\n",
    "        return \"warehouse\"\n",
    "    else:\n",
    "        return \"shipping\"\n",
    "\n",
    "def get_tools_for_intent(intent: str) -> list:\n",
    "    \"\"\"Return tools for intent.\"\"\"\n",
    "    if intent == \"shipping\":\n",
    "        return shipping_core_tools + carrier_tools\n",
    "    elif intent == \"returns\":\n",
    "        return shipping_core_tools + returns_tools\n",
    "    elif intent == \"billing\":\n",
    "        return shipping_core_tools + billing_tools\n",
    "    elif intent == \"warehouse\":\n",
    "        return shipping_core_tools + warehouse_tools\n",
    "    return shipping_core_tools\n",
    "\n",
    "print(\"âœ“ Created intent-based router\")\n",
    "\n",
    "# Test routed agent\n",
    "def run_routed_agent_with_trajectory(inputs: dict) -> dict:\n",
    "    \"\"\"Run routed agent with intent-based tool selection and return structured output.\"\"\"\n",
    "    query = inputs[\"query\"]\n",
    "    intent = classify_query_intent(query)\n",
    "    tools = get_tools_for_intent(intent)\n",
    "    \n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=tools,\n",
    "        system_prompt=SHIPPING_SUPPORT_INSTRUCTIONS\n",
    "    )\n",
    "    \n",
    "    return run_agent_with_trajectory(agent, query)\n",
    "\n",
    "routed_results = evaluate(\n",
    "    run_routed_agent_with_trajectory,\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"solution-routed\",\n",
    "    metadata={\"config\": \"routed\"},\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Routed agent evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Routing vs Noise\n",
    "\n",
    "How does smart routing compare to the worst-case noise scenario?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare routing solution vs worst noise configuration\n",
    "try:\n",
    "    worst_noise_metrics = get_metrics_from_experiment(\"noise-25-pct-relevant\")\n",
    "    routed_metrics = get_metrics_from_experiment(\"solution-routed\")\n",
    "    \n",
    "    routing_comparison = {\n",
    "        \"Configuration\": [\"25% Relevant Tools (No Routing)\", \"Smart Routing\"],\n",
    "        \"Trajectory Match\": [worst_noise_metrics.get(\"trajectory_match\", 0), routed_metrics.get(\"trajectory_match\", 0)],\n",
    "        \"Correctness\": [worst_noise_metrics.get(\"correctness\", 0), routed_metrics.get(\"correctness\", 0)],\n",
    "        \"Tool Efficiency\": [worst_noise_metrics.get(\"tool_efficiency\", 0), routed_metrics.get(\"tool_efficiency\", 0)],\n",
    "    }\n",
    "    \n",
    "    # Create bar chart comparison\n",
    "    df_routing = pd.DataFrame(routing_comparison)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='25% Relevant (No Routing)',\n",
    "        x=['Trajectory Match', 'Correctness'],\n",
    "        y=[worst_noise_metrics.get(\"trajectory_match\", 0), worst_noise_metrics.get(\"correctness\", 0)],\n",
    "        marker_color='#d62728'\n",
    "    ))\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Smart Routing',\n",
    "        x=['Trajectory Match', 'Correctness'],\n",
    "        y=[routed_metrics.get(\"trajectory_match\", 0), routed_metrics.get(\"correctness\", 0)],\n",
    "        marker_color='#2ca02c'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Routing Solution vs Worst Noise Configuration\",\n",
    "        yaxis_title=\"Score\",\n",
    "        barmode='group',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    display(df_routing)\n",
    "    \n",
    "    print(\"\\nâœ… Smart routing eliminates noise:\")\n",
    "    if routed_metrics.get(\"trajectory_match\", 0) > 0:\n",
    "        improvement = ((routed_metrics.get(\"trajectory_match\", 0) - worst_noise_metrics.get(\"trajectory_match\", 0)) / worst_noise_metrics.get(\"trajectory_match\", 1) * 100) if worst_noise_metrics.get(\"trajectory_match\", 0) > 0 else 0\n",
    "        print(f\"   Trajectory match improvement: {improvement:.0f}%\")\n",
    "        print(f\"   Correctness improved from {worst_noise_metrics.get('correctness', 0):.2f} to {routed_metrics.get('correctness', 0):.2f}\")\n",
    "    else:\n",
    "        print(\"   Run evaluations above to see actual metrics\")\n",
    "    print(\"   - Intent classification filters out irrelevant tool domains\")\n",
    "    print(\"   - Agent sees only tools needed for the query type\")\n",
    "    print(\"   - Maintains performance by keeping context focused\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Run the evaluations above first to see comparison. Error: {e}\")\n",
    "    routing_comparison = {\n",
    "        \"Configuration\": [\"25% Relevant Tools (No Routing)\", \"Smart Routing\"],\n",
    "        \"Trajectory Match\": [\"Run eval\", \"Run eval\"],\n",
    "        \"Correctness\": [\"Run eval\", \"Run eval\"],\n",
    "        \"Tool Efficiency\": [\"Run eval\", \"Run eval\"],\n",
    "    }\n",
    "    df_routing = pd.DataFrame(routing_comparison)\n",
    "    display(df_routing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 3: Instruction Bloat Hurts Too\n",
    "\n",
    "**Hypothesis:** Verbose instructions degrade performance.\n",
    "\n",
    "Add evaluator to check instruction following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected tools checker is now replaced by trajectory_match_evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with bloated instructions\n",
    "clean_instructions = f\"{SHIPPING_SUPPORT_INSTRUCTIONS}\\n\\n{CARRIER_MANAGEMENT_INSTRUCTIONS}\"\n",
    "noisy_instructions = f\"{clean_instructions}\\n\\n{IRRELEVANT_INSTRUCTIONS}\"\n",
    "\n",
    "print(f\"Clean: {len(clean_instructions):,} chars\")\n",
    "print(f\"Noisy: {len(noisy_instructions):,} chars\")\n",
    "print(f\"Bloat: {len(noisy_instructions) / len(clean_instructions):.1f}x\")\n",
    "\n",
    "noisy_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=shipping_core_tools,\n",
    "    system_prompt=noisy_instructions\n",
    ")\n",
    "\n",
    "evaluate(\n",
    "    lambda inputs: run_agent_with_trajectory(noisy_agent, inputs[\"query\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=ALL_EVALUATORS,\n",
    "    experiment_prefix=\"problem-noisy-instructions\",\n",
    "    metadata={\"tool_count\": 6, \"instruction_length\": len(noisy_instructions)},\n",
    ")\n",
    "\n",
    "print(\"âœ“ Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Instruction Bloat Impact\n",
    "\n",
    "Let's see how extra instructions affected performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and compare instruction bloat impact\n",
    "try:\n",
    "    clean_baseline_metrics = get_metrics_from_experiment(\"scaling-6-tools\")\n",
    "    bloated_metrics = get_metrics_from_experiment(\"problem-noisy-instructions\")\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    metrics_to_compare = ['Trajectory Match', 'Correctness', 'Tool Efficiency']\n",
    "    clean_scores = [\n",
    "        clean_baseline_metrics.get(\"trajectory_match\", 0),\n",
    "        clean_baseline_metrics.get(\"correctness\", 0),\n",
    "        clean_baseline_metrics.get(\"tool_efficiency\", 1.0)  # Baseline expected to be 1.0\n",
    "    ]\n",
    "    bloated_scores = [\n",
    "        bloated_metrics.get(\"trajectory_match\", 0),\n",
    "        bloated_metrics.get(\"correctness\", 0),\n",
    "        bloated_metrics.get(\"tool_efficiency\", 0)\n",
    "    ]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Clean Instructions',\n",
    "        x=metrics_to_compare,\n",
    "        y=clean_scores,\n",
    "        marker_color='#2ca02c'\n",
    "    ))\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Bloated Instructions',\n",
    "        x=metrics_to_compare,\n",
    "        y=bloated_scores,\n",
    "        marker_color='#d62728'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Impact of Instruction Bloat on Agent Performance\",\n",
    "        yaxis_title=\"Score\",\n",
    "        barmode='group',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create comparison table\n",
    "    instruction_impact = {\n",
    "        \"Metric\": [\"Trajectory Match\", \"Correctness\", \"Tool Efficiency\"],\n",
    "        \"Clean Instructions\": [\n",
    "            f\"{clean_baseline_metrics.get('trajectory_match', 0):.2f}\",\n",
    "            f\"{clean_baseline_metrics.get('correctness', 0):.2f}\",\n",
    "            f\"{clean_baseline_metrics.get('tool_efficiency', 1.0):.2f}\"\n",
    "        ],\n",
    "        \"Bloated Instructions\": [\n",
    "            f\"{bloated_metrics.get('trajectory_match', 0):.2f}\",\n",
    "            f\"{bloated_metrics.get('correctness', 0):.2f}\",\n",
    "            f\"{bloated_metrics.get('tool_efficiency', 0):.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_instructions = pd.DataFrame(instruction_impact)\n",
    "    display(df_instructions)\n",
    "    \n",
    "    print(\"\\nðŸ“‰ Key Findings on Instruction Bloat:\")\n",
    "    if bloated_metrics.get(\"trajectory_match\", 0) > 0:\n",
    "        traj_drop = ((clean_baseline_metrics.get(\"trajectory_match\", 0) - bloated_metrics.get(\"trajectory_match\", 0)) / clean_baseline_metrics.get(\"trajectory_match\", 1) * 100) if clean_baseline_metrics.get(\"trajectory_match\", 0) > 0 else 0\n",
    "        corr_drop = ((clean_baseline_metrics.get(\"correctness\", 0) - bloated_metrics.get(\"correctness\", 0)) / clean_baseline_metrics.get(\"correctness\", 1) * 100) if clean_baseline_metrics.get(\"correctness\", 0) > 0 else 0\n",
    "        print(f\"   Trajectory match degraded by {traj_drop:.0f}%\")\n",
    "        print(f\"   Correctness degraded by {corr_drop:.0f}%\")\n",
    "        print(f\"   Tool efficiency: {bloated_metrics.get('tool_efficiency', 0):.2f}\")\n",
    "    else:\n",
    "        print(\"   Run evaluations above to see actual metrics\")\n",
    "    print(\"   - Extra instructions from irrelevant domains confuse the agent\")\n",
    "    print(\"   - Agent may miss obvious tool choices\")\n",
    "    print(\"   - Focused instructions lead to better tool selection\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Run the evaluations above first to see comparison. Error: {e}\")\n",
    "    instruction_impact = {\n",
    "        \"Metric\": [\"Trajectory Match\", \"Correctness\", \"Tool Efficiency\"],\n",
    "        \"Clean Instructions\": [\"Run eval\", \"Run eval\", \"Run eval\"],\n",
    "        \"Bloated Instructions\": [\"Run eval\", \"Run eval\", \"Run eval\"]\n",
    "    }\n",
    "    df_instructions = pd.DataFrame(instruction_impact)\n",
    "    display(df_instructions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solution 3: Instruction Pruning\n",
    "\n",
    "**Insight:** Keep only essential instructions. LangSmith A/B testing validates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and compare\n",
    "try:\n",
    "    clean_metrics = get_metrics_from_experiment(\"scaling-6-tools\")\n",
    "    noisy_metrics = get_metrics_from_experiment(\"problem-noisy-instructions\")\n",
    "    \n",
    "    instruction_comparison = {\n",
    "        \"Instructions\": [\"Clean (focused)\", \"Bloated (all domains)\"],\n",
    "        \"Tool Count\": [6, 6],\n",
    "        \"Trajectory Match\": [clean_metrics.get(\"trajectory_match\", 0), noisy_metrics.get(\"trajectory_match\", 0)],\n",
    "        \"Correctness\": [clean_metrics.get(\"correctness\", 0), noisy_metrics.get(\"correctness\", 0)],\n",
    "    }\n",
    "except:\n",
    "    instruction_comparison = {\n",
    "        \"Instructions\": [\"Clean (focused)\", \"Bloated (all domains)\"],\n",
    "        \"Tool Count\": [6, 6],\n",
    "        \"Trajectory Match\": [\"Run eval\", \"Run eval\"],\n",
    "        \"Correctness\": [\"Run eval\", \"Run eval\"],\n",
    "    }\n",
    "\n",
    "df_instructions = pd.DataFrame(instruction_comparison)\n",
    "display(df_instructions)\n",
    "\n",
    "print(\"\\nâœ… Minimal instructions maintain or improve performance\")\n",
    "print(\"   - LangSmith A/B testing validates focused instructions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### The Sweet Spot Is Real\n",
    "\n",
    "Our experiments demonstrate a clear U-curve:\n",
    "\n",
    "- **75 tools (overload):** Context confusion from overlapping choices leads to poor performance\n",
    "- **6 tools (minimal):** Over-correction leaves gaps - can't handle all query types\n",
    "- **12 tools (consolidated):** Sweet spot with flexible parameters delivers full capability AND clarity\n",
    "\n",
    "**The winning approach:** Intelligent consolidation at the right level of abstraction.\n",
    "\n",
    "### What Makes Consolidated Tools Work\n",
    "\n",
    "1. **Flexible parameters over many specific tools** - One `get_order_info` with `include` parameter beats five separate getter functions\n",
    "\n",
    "2. **Reduces cognitive load** - 12 well-designed tools are easier to navigate than 75 overlapping options\n",
    "\n",
    "3. **Maintains full capability** - Parameter-based design covers all use cases without sacrificing functionality\n",
    "\n",
    "4. **Measurable with LangSmith** - Custom evaluators (trajectory match, correctness, efficiency) quantify the improvement\n",
    "\n",
    "### Professional Implementation Strategy\n",
    "\n",
    "When facing context confusion in production:\n",
    "\n",
    "1. **Start with measurement** - Use LangSmith to quantify the problem (don't guess)\n",
    "2. **Identify tool clusters** - Group tools that serve similar purposes\n",
    "3. **Design consolidated versions** - Add parameters for flexibility instead of creating new tools\n",
    "4. **Validate with evals** - Ensure consolidated tools match or exceed original performance\n",
    "5. **Iterate to sweet spot** - Find the balance between too many and too few\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "By finding the sweet spot:\n",
    "- **Better performance** - Higher accuracy, fewer mistakes from tool confusion\n",
    "- **Lower costs** - Fewer tokens in context, faster inference\n",
    "- **Easier maintenance** - Consolidated, well-organized toolsets\n",
    "- **Better UX** - Faster, more accurate responses for users\n",
    "- **Scalable** - Room to add capabilities without degrading performance\n",
    "\n",
    "---\n",
    "\n",
    "**Context confusion isn't solved by minimalism - it's solved by intelligent design. Use LangSmith to measure, then optimize to your sweet spot.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
